{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importando Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import pickle\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.exceptions import ConvergenceWarning, UndefinedMetricWarning\n",
    "\n",
    "import mnist_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregando o MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading MNIST dataset\n",
    "training_data, validation_data, test_data = mnist_loader.load_data()\n",
    "\n",
    "X_train, Y_train = np.concatenate((training_data[0], validation_data[0])), np.concatenate((training_data[1], validation_data[1]))\n",
    "X_test, Y_test = test_data[0], test_data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definindo os parâmetros da MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os parâmetros testados em busca do melhor desempenho estão descritos abaixo:\n",
    "<br>\n",
    "- Número de camadas: Definido aleatoriamente para cada experimento, de forma que cada MLP testado terá 1 ou mais camadas ocultas, com cada camada sendo adicionada caso o número aleatório retornado pelo random.uniform seja maior do que 0.4\n",
    "- Número de neurônios: Também definido aleatoriamente para cada experimento e camada, sendo um número entre 2 e 100\n",
    "- Taxa de aprendizagem: Número definido de forma aleatória entre 0.001 e 0.1, tendo no máximo 4 dígitos\n",
    "- Função de ativação: Escolhido aleatoriamente, podendo ser 'identity', 'logistic', 'tanh' ou 'relu'\n",
    "- Algoritmo de aprendizagem: Também escolhido aleatoriamente, podendo ser 'adam' (Adaptive Moment Estimation), 'sgd' (Stochastic Gradient Descent) ou 'lbfgs' (Limited-memory Broyden-Fletcher-Goldfarb-Shanno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir a quantidade de camadas escondidas\n",
    "def random_hidden_layers():\n",
    "    hidden_layer = [random.randint(2, 100)]\n",
    "    while random.uniform(0.0, 1.0) > 0.4:\n",
    "        hidden_layer.append(random.randint(2, 100))\n",
    "    return hidden_layer\n",
    "\n",
    "def random_params_mlp(verbose=False, num_epochs=100, tol=0.0001):\n",
    "    return MLPClassifier(\n",
    "        activation=random.choice(['identity', 'logistic', 'tanh', 'relu']),\n",
    "        batch_size='auto',\n",
    "        early_stopping=True,\n",
    "        hidden_layer_sizes=random_hidden_layers(),\n",
    "        learning_rate_init=round(random.uniform(0.0001, 0.1), 4),\n",
    "        max_iter=num_epochs,\n",
    "        n_iter_no_change=3,\n",
    "        solver=random.choice(['adam', 'sgd', 'lbfgs']),\n",
    "        tol=tol,\n",
    "        verbose=verbose,\n",
    "        warm_start=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fazendo um \"GridSearch\" sobre a camada escondida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiro vamos definir a estrutura do dataframe para armazenar os resultados obtidos durante o gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (os.path.exists('model_metrics.csv')):\n",
    "    model_df = pd.read_csv('model_metrics.csv')\n",
    "else:\n",
    "    model_data = {\n",
    "        'solver': [],\n",
    "        'activation_function': [],\n",
    "        'hidden_layers': [],\n",
    "        'learning_rate': [],\n",
    "        'accuracy': [],\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1_score': [],\n",
    "        'fit_time': [],\n",
    "        'total_epochs': [],\n",
    "    }\n",
    "    model_df = pd.DataFrame(model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em seguida, realizamos 10 experimentos, cada um com parâmetros diferentes escolhidos aleatoriamente usando a biblioteca random, que é nativa do python. Para cada experimento, iremos exibir a acurácia total e tempo de treinamento do modelo testado, além de salvar seus resultados (acurácia total, precisão total, recall total, tempo de treinamento e f1 score) e parâmetros (função de ativação, algoritmo de aprendizagem, número de epochs, número de camadas e neurônios e taxa de aprendizagem).\n",
    "<br>No final de cada experimento, são salvos os três modelos com a maior acurácia total, os quais são exibidos após a conclusão de todos os 10 testes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_models = []\n",
    "\n",
    "print(\"Activation | Solver | LR | Hidden Layers | Total Time | Total Epochs -> Accuracy\")\n",
    "for _ in range(10):\n",
    "    mlp = random_params_mlp(num_epochs=250)\n",
    "\n",
    "    start = time.time()\n",
    "    mlp.fit(X_train, Y_train)\n",
    "    end = time.time()\n",
    "    total_time = round(end - start, 1)\n",
    "\n",
    "    mlp_predictions = mlp.predict(X_test)\n",
    "    accuracy = accuracy_score(Y_test, mlp_predictions)\n",
    "    precision = precision_score(Y_test, mlp_predictions, average='macro')\n",
    "    recall = recall_score(Y_test, mlp_predictions, average='macro')\n",
    "    f1 = f1_score(Y_test, mlp_predictions, average='macro')\n",
    "    print(f\"{mlp.get_params()['activation']} | {mlp.get_params()['solver']} | {mlp.get_params()['learning_rate_init']} \\\n",
    "| {mlp.get_params()['hidden_layer_sizes']} | {total_time}s | {mlp.n_iter_} -> {accuracy}\")\n",
    "\n",
    "    trained_models.append((mlp, accuracy))\n",
    "    trained_models = sorted(trained_models, key=lambda tup: tup[1], reverse=True)[:3]\n",
    "\n",
    "    # Adicionando o modelo ao dataframe de modelos\n",
    "    model_df.loc[len(model_df)] = {\n",
    "        'solver': mlp.get_params()['solver'],\n",
    "        'activation_function': mlp.get_params()['activation'],\n",
    "        'hidden_layers': str(mlp.get_params()['hidden_layer_sizes']),\n",
    "        'learning_rate': mlp.get_params()['learning_rate_init'],\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'fit_time': total_time,\n",
    "        'total_epochs': mlp.n_iter_,\n",
    "    }\n",
    "\n",
    "    model_df.to_csv('model_metrics.csv', index=False)\n",
    "\n",
    "print(\"\\nTop 3 Models:\")\n",
    "for model, acc in trained_models:\n",
    "    print(f\"Accuracy: {acc}, Params: {model.get_params()['hidden_layer_sizes']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df.to_csv('model_metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acima temos os parâmetros para cada um dos 25 Multilayer Perceptrons testados, junto com sua acurácia total e o tempo que levou para treinar cada um deles. <br>\n",
    "Nota-se que a maioria dos modelos apresentam uma acurácia parecida, com excessão de alguns modelos que possuem 4 camadas ocultas, com o restante dos modelos apresentando uma acurácia total maior do que 0.89."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise da Performance Sobre o Conjunto de Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = trained_models[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_predictions = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculando Acurácia, Precisão e Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feita a previsão do modelo com os dados do conjunto de teste, foi calculado a acurácia, precisão, recall total e f1 score  usando as funções accuracy_score, precision_score, recall_score e f1_score, todos da biblioteca scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(Y_test, mlp_predictions)\n",
    "precision = precision_score(Y_test, mlp_predictions, average='macro')\n",
    "recall = recall_score(Y_test, mlp_predictions, average='macro')\n",
    "f1 = f1_score(Y_test, mlp_predictions, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Além disso, exibimos as principais métricas de classificação (acurácia, precisão, recall e f1 score) para cada classe para que possamos avaliar a confiabilidade do modelo na classificação de cada dígito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pegando o report de cada classe \n",
    "report = classification_report(Y_test, mlp_predictions, target_names=['0','1','2','3','4','5','6','7','8','9'], output_dict = True)\n",
    "conf_matrix = confusion_matrix(Y_test, mlp_predictions )\n",
    "# Calcular a acurácia para cada classe\n",
    "accuracy_per_class = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
    "for i in range(10):\n",
    "  report[str(i)]['accuracy'] = accuracy_per_class[i]\n",
    "\n",
    "#Printando a tabela\n",
    "df = pd.DataFrame(report)\n",
    "df_inverted = df.transpose()\n",
    "df = df.drop(columns=['accuracy', 'macro avg', 'weighted avg'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotando a Matrix de Confusão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Também exibimos a matriz de confusão abaixo, fornecendo uma outra visão da performance do melhor modelo para cada uma das suas 10 classes, de forma a avaliarmos a quantidade de instâncias de cada classe que o modelo classificou corretamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(Y_test, mlp_predictions)\n",
    "plt.figure(figsize=(8, 7))\n",
    "\n",
    "# Heatmap\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, xticklabels=range(10), yticklabels=range(10))\n",
    "\n",
    "# Labels\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "\n",
    "for i in range(11):\n",
    "    plt.hlines(i, xmin=0, xmax=10, colors='black', linestyles='solid', linewidth=1)\n",
    "    plt.vlines(i, ymin=0, ymax=10, colors='black', linestyles='solid', linewidth=1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observa-se que o modelo tem uma performance boa em todas as classes, acertando a maioria dos casos de teste, errando principalmente os dígitos 8 e 9, onde mais de 30 instâncias foram classificadas incorretamente, porém com mais de 900 instâncias, para cada um desses dígitos, sendo classificadas corretamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Média e Desvio Padrão das Métricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, iremos calcular a média e desvio padrão dos 3 melhores conjuntos de parâmetros para ter um melhor entendimento do quão bom é a performance dos modelos treinados com esses parâmetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'Params': [],\n",
    "    'Mean Accuracy': [],\n",
    "    'Std Accuracy': [],\n",
    "    'Mean Precision': [],\n",
    "    'Std Precision': [],\n",
    "    'Mean Recall': [],\n",
    "    'Std Recall': [],\n",
    "    'Mean Time': [],\n",
    "    'Std Time': [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando 10 modelos para as 3 melhores combinações de parâmetros\n",
    "for i, model in enumerate(trained_models):\n",
    "    model = model[0]\n",
    "    accuracy = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    fit_time = []\n",
    "    for _ in range(10):\n",
    "        start = time.time()\n",
    "        model.fit(X_train, Y_train)\n",
    "        end = time.time()\n",
    "        fit_time.append(round(end - start, 1))\n",
    "\n",
    "        model_predictions = model.predict(X_test)\n",
    "        accuracy.append(accuracy_score(Y_test, model_predictions))\n",
    "        precision.append(precision_score(Y_test, model_predictions, average='weighted'))\n",
    "        recall.append(recall_score(Y_test, model_predictions, average='weighted'))\n",
    "\n",
    "    data['Params'].append(f\"C{i}\")\n",
    "    data['Mean Accuracy'].append(np.mean(accuracy))\n",
    "    data['Std Accuracy'].append(np.std(accuracy))\n",
    "    data['Mean Precision'].append(np.mean(precision))\n",
    "    data['Std Precision'].append(np.std(precision))\n",
    "    data['Mean Recall'].append(np.mean(recall))\n",
    "    data['Std Recall'].append(np.std(recall))\n",
    "    data['Mean Time'].append(np.mean(fit_time))\n",
    "    data['Std Time'].append(np.std(fit_time))\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salvando o melhor modelo treinado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nessa secção está o código para salvar o modelo de melhor performance usando o pickle, outra biblioteca nativa do python.\n",
    "<br>Salvamos esse modelo em um arquivo .pkl que pode ser aberto posteriormente com o pickle.load(open(caminho_arquivo_pkl, 'rb'))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'test_model.plk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando o modelo em um arquivo\n",
    "with open(filename, 'wb') as f:\n",
    "    pickle.dump(best_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Carregando um modelo salvo\n",
    "# with open(filename, 'rb') as f:\n",
    "#     best_model = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
