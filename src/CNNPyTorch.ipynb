{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os, datetime, torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms \n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando e seperando dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# Transformações para normalizar o dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Carregando o dataset MNIST\n",
    "trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# DataLoader para o dataset de treinamento e teste\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "classes = list(map(str, range(0, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualizando dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(dl):\n",
    "    for images, labels in dl:\n",
    "        fig, ax = plt.subplots(figsize=(12, 12))\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "        ax.imshow(make_grid(images[:64], nrow=8).permute(1, 2, 0))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x1200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_batch(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x1200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_batch(testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando uma instância da classe nn.Module para criar redes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explicando cada parâmetro da camada convolucional:\n",
    "\n",
    "- **in_channels**: Quantos canais de cores os inputs possuem. No caso de imagens em preto e branco, como estamos trabalhando com MNIST, há apenas um canal de cor.\n",
    "\n",
    "- **out_channels**: Número de filtros (kernels) que serão aplicados à imagem durante a convolução. Cada filtro é responsável por extrair características latentes da imagem.\n",
    "\n",
    "- **kernel_size**: Dimensão do filtro utilizado na convolução. O valor comum de 3 é amplamente usado, pois é suficiente para capturar detalhes locais, ao mesmo tempo em que percebe padrões maiores na imagem.\n",
    "\n",
    "- **stride**: Indica de quanto em quantos pixels o filtro será aplicado na imagem.\n",
    "\n",
    "- **padding**: Adiciona pixels em volta da imagem de entrada durante a convolução para garantir que o tamanho da saída após a convolução permaneça o mesmo que o da entrada.\n",
    "  - *Observação*: Ao adicionar padding, é preciso ter cuidado com os valores para garantir que a resolução e o tamanho da imagem não sejam afetados de forma indesejada.\n",
    "\n",
    "- **dropout_prob**: Probabilidade de que um neurônio seja desligado durante o treinamento da rede. Isso é uma técnica de regularização que ajuda a prevenir o overfitting, forçando a rede a aprender representações mais robustas e generalizáveis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fórmulas das Dimensões de Saída\n",
    "\n",
    "A fórmula para calcular a dimensão de saída \\( W_out \\) da convolução é:\n",
    "\n",
    "$$\n",
    "W_{\\text{out}} = \\left\\lfloor \\frac{W_{\\text{in}} + 2 \\times \\text{padding} - \\text{kernel\\_size}}{\\text{stride}} \\right\\rfloor + 1\n",
    "$$\n",
    "\n",
    "E a fórmula para calcular a dimensão de saída \\( H_out \\) da convolução é:\n",
    "\n",
    "$$\n",
    "H_{\\text{out}} = \\left\\lfloor \\frac{H_{\\text{in}} + 2 \\times \\text{padding} - \\text{kernel\\_size}}{\\text{stride}} \\right\\rfloor + 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resumindo as trasnformações:\n",
    "\n",
    "- Entrada Original: 28x28 pixels, 1 canal.\n",
    "- Após Convolução: 28x28 pixels, 32 canais.\n",
    "- Após Pooling: 14x14 pixels, 32 canais.\n",
    "- Entrada para self.fc1: 32 × 14 × 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseCNN(nn.Module):\n",
    "    def __init__(self, conv_kernel_size=3, conv_stride=1, conv_padding=1, pool_kernel_size=2, pool_stride=2, dropout_prob=0.5):\n",
    "        super(BaseCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=conv_kernel_size, stride=conv_stride, padding=conv_padding)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=pool_kernel_size, stride=pool_stride)\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        self.fc1 = nn.Linear(32 * 14 * 14, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(-1, 32 * 14 * 14)\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepCNN(nn.Module):\n",
    "    def __init__(self, conv_kernel_size=3, conv_stride=1, conv_padding=1, pool_kernel_size=2, pool_stride=2, dropout_prob=0.5):\n",
    "        super(DeepCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=conv_kernel_size, stride=conv_stride, padding=conv_padding)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=conv_kernel_size, stride=conv_stride, padding=conv_padding)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=pool_kernel_size, stride=pool_stride)\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, trial, num_conv_layers, num_filters, num_neurons, drop_conv2, drop_prob_fc1):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        in_size = 28\n",
    "        conv_kernel_size = 3\n",
    "        # conv_kernel_stride\n",
    "        pool_kernel_size = 2\n",
    "        # pool_stride\n",
    "\n",
    "        # TODO(pedro): Try to make both the conv and pool kernel sizes constructor parameters.\n",
    "\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, num_filters[0],\n",
    "                                              kernel_size=(conv_kernel_size, conv_kernel_size))])\n",
    "        out_size = int((in_size - conv_kernel_size + 1) / 2)\n",
    "\n",
    "        for i in range(1, num_conv_layers):\n",
    "            self.convs.append(nn.Conv2d(in_channels=num_filters[i - 1], out_channels=num_filters[i],\n",
    "                                        kernel_size=(conv_kernel_size, conv_kernel_size)))\n",
    "            out_size = int((out_size - conv_kernel_size + 1) / 2)\n",
    "\n",
    "        self.conv2_drop = nn.Dropout2d(p=drop_conv2)\n",
    "        self.out_feature = num_filters[num_conv_layers - 1] * out_size * out_size\n",
    "        self.pool = nn.MaxPool2d(kernel_size=pool_kernel_size)\n",
    "        self.fc1 = nn.Linear(self.out_feature, num_neurons)\n",
    "        self.fc2 = nn.Linear(num_neurons, 10)\n",
    "        self.fc1_drop_prob = drop_prob_fc1\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x)\n",
    "            if i == 2:\n",
    "                x = self.conv2_drop(x)\n",
    "            x = F.relu(self.pool(x))\n",
    "\n",
    "        x = x.view(-1, self.out_feature)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, p=self.fc1_drop_prob, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definindo funções úteis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Função de treinamento do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, epochs):\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(trainloader):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()  # Propaga os gradientes\n",
    "            optimizer.step()  # Atualiza os pesos\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f'Epoch [{epoch + 1}/{epochs}], Step [{i + 1}/{len(trainloader)}], Loss: {running_loss / len(trainloader)}')\n",
    "    \n",
    "    print('Finished Training!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Função de teste do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, criterion):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testloader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    accuracy = correct / total\n",
    "    loss = test_loss / len(testloader)\n",
    "    precision = precision_score(all_labels, all_predictions, average='weighted')\n",
    "    recall = recall_score(all_labels, all_predictions, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "\n",
    "    return accuracy, loss, precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Função de acurácia de cada classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mudar\n",
    "def accuracy_classes(model_trained):\n",
    "    correct_pred = {classname: 0 for classname in classes}\n",
    "    total_pred = {classname: 0 for classname in classes}\n",
    "    all_accuracy = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            outputs = model_trained(images)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            # collect the correct predictions for each class\n",
    "            for label, prediction in zip(labels, predictions):\n",
    "                if label == prediction:\n",
    "                    correct_pred[classes[label]] += 1\n",
    "                total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "    for classname, correct_count in correct_pred.items():\n",
    "        accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "        all_accuracy.append(accuracy)\n",
    "    \n",
    "    return all_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Função para prever classe de uma imagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(image_path, model):\n",
    "    image = Image.open(image_path).convert('L')  \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((28, 28)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    # Adiciona dimensão de batch (1 imagem)\n",
    "    image = transform(image).unsqueeze(0) \n",
    "\n",
    "    # Passando a imagem pela rede\n",
    "    output = model(image)\n",
    "\n",
    "    _, predicted_class = torch.max(output, 1)\n",
    "    print(\"Classe prevista:\", predicted_class.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definindo cvs com os parametros e metricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (os.path.exists('cnn_models_metrics.csv')):\n",
    "    model_df = pd.read_csv('cnn_models_metrics.csv')\n",
    "else:\n",
    "    model_data = {\n",
    "        'network': [],\n",
    "        'conv_kernel_size': [],\n",
    "        'conv_stride': [],\n",
    "        'conv_padding': [],\n",
    "        'pool_kernel_size': [],\n",
    "        'pool_stride': [],\n",
    "        'dropout_prob': [],\n",
    "        'fit_time': [],\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1_score': [],\n",
    "        'overall_accuracy ': [],\n",
    "        'accuracy_0': [],\n",
    "        'accuracy_1': [],\n",
    "        'accuracy_2': [],\n",
    "        'accuracy_3': [],\n",
    "        'accuracy_4': [],\n",
    "        'accuracy_5': [],\n",
    "        'accuracy_6': [],\n",
    "        'accuracy_7': [],\n",
    "        'accuracy_8': [],\n",
    "        'accuracy_9': [],\n",
    "        'total_epochs': [],\n",
    "        'learning_rate': [],\n",
    "    }\n",
    "    model_df = pd.DataFrame(model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-21 06:05:08,278] A new study created in memory with name: no-name-16d68f62-5551-4319-b039-04eca3d76713\n",
      "/tmp/ipykernel_23462/3491431137.py:31: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  num_filters = [int(trial.suggest_discrete_uniform(\"num_filter_\" + str(i), 16, 128, 16))\n",
      "/tmp/ipykernel_23462/3491431137.py:33: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  num_neurons = trial.suggest_int(\"num_neurons\", 10, 400, 10)  # Number of neurons of fully connected layer 1\n",
      "/home/pedro/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 2.3857, Test Loss: 2.3021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss: 2.3048, Test Loss: 2.3022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Loss: 2.3046, Test Loss: 2.3061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train Loss: 2.3048, Test Loss: 2.3044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Train Loss: 2.3049, Test Loss: 2.3051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Train Loss: 2.3049, Test Loss: 2.3036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Train Loss: 2.3045, Test Loss: 2.3046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Train Loss: 2.3045, Test Loss: 2.3046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Train Loss: 2.3048, Test Loss: 2.3051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "[I 2024-05-21 06:23:23,861] Trial 0 finished with value: 0.1028 and parameters: {'num_conv_layers': 2, 'num_filter_0': 80.0, 'num_filter_1': 112.0, 'num_neurons': 260, 'drop_conv2': 0.2834073203180282, 'drop_prob_fc1': 0.4654439629372404, 'optimizer': 'Adam', 'lr': 0.027038071212159218}. Best is trial 0 with value: 0.1028.\n",
      "/tmp/ipykernel_23462/3491431137.py:31: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  num_filters = [int(trial.suggest_discrete_uniform(\"num_filter_\" + str(i), 16, 128, 16))\n",
      "/tmp/ipykernel_23462/3491431137.py:33: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  num_neurons = trial.suggest_int(\"num_neurons\", 10, 400, 10)  # Number of neurons of fully connected layer 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Train Loss: 2.3048, Test Loss: 2.3084\n",
      "Epoch 1, Train Loss: 1.1953, Test Loss: 0.2178\n",
      "Epoch 2, Train Loss: 0.4434, Test Loss: 0.1548\n",
      "Epoch 3, Train Loss: 0.3273, Test Loss: 0.1489\n",
      "Epoch 4, Train Loss: 0.2981, Test Loss: 0.1459\n",
      "Epoch 5, Train Loss: 0.3134, Test Loss: 0.1651\n",
      "Epoch 6, Train Loss: 0.3343, Test Loss: 0.1749\n",
      "Epoch 7, Train Loss: 0.2829, Test Loss: 0.1966\n",
      "Epoch 8, Train Loss: 0.2917, Test Loss: 0.1906\n",
      "Epoch 9, Train Loss: 0.2962, Test Loss: 0.2402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-21 06:44:09,743] Trial 1 finished with value: 0.9622 and parameters: {'num_conv_layers': 1, 'num_filter_0': 112.0, 'num_neurons': 160, 'drop_conv2': 0.3575585061837717, 'drop_prob_fc1': 0.3464457608509821, 'optimizer': 'Adam', 'lr': 0.019189759099290084}. Best is trial 1 with value: 0.9622.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Train Loss: 0.2856, Test Loss: 0.1978\n",
      "\n",
      "-- Study Statistics --\n",
      "  Number of finished trials: 2\n",
      "  Number of pruned trials:   0\n",
      "  Number of complete trials: 2\n",
      "\n",
      "-- Best Trial --\n",
      "  Accuracy:  0.9622\n",
      "  Test Loss: 0.19777703904342583\n",
      "  Precision: 0.9639595354121496\n",
      "  Recall:    0.9622\n",
      "  F1 Score:  0.9624155757569226\n",
      "  Parameters: \n",
      "    num_conv_layers: 1\n",
      "    num_filter_0:    112.0\n",
      "    num_neurons:     160\n",
      "    drop_conv2:      0.3575585061837717\n",
      "    drop_prob_fc1:   0.3464457608509821\n",
      "    optimizer:       Adam\n",
      "    lr:              0.019189759099290084\n",
      "\n",
      "-- Overall Results (Ordered by Accuracy) --\n",
      "   number   value  params_drop_conv2  params_drop_prob_fc1  params_lr  \\\n",
      "0       0  0.1028           0.283407              0.465444   0.027038   \n",
      "1       1  0.9622           0.357559              0.346446   0.019190   \n",
      "\n",
      "   params_num_conv_layers  params_num_filter_0  params_num_filter_1  \\\n",
      "0                       2                 80.0                112.0   \n",
      "1                       1                112.0                  NaN   \n",
      "\n",
      "   params_num_neurons params_optimizer  user_attrs_f1_score  \\\n",
      "0                 260             Adam             0.019165   \n",
      "1                 160             Adam             0.962416   \n",
      "\n",
      "   user_attrs_precision  user_attrs_recall  user_attrs_test_loss  \n",
      "0              0.010568             0.1028              2.308376  \n",
      "1              0.963960             0.9622              0.197777  \n",
      "\n",
      "-- Most Important Hyperparameters --\n",
      "  lr:              33.33%\n",
      "  num_neurons:     26.67%\n",
      "  num_filter_0:    18.67%\n",
      "  drop_prob_fc1:   14.67%\n",
      "  num_conv_layers: 6.67%\n",
      "  optimizer:       0.00%\n",
      "  drop_conv2:      0.00%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "num_trials = 2\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# TODO(pedro): Try to generate different types of criterion inside the objective function.\n",
    "\n",
    "# TODO(pedro): Factor out this function eventually.\n",
    "def train_model_once(model, criterion, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in trainloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()  # Propaga os gradientes\n",
    "        optimizer.step()  # Atualiza os pesos\n",
    "\n",
    "        running_loss += loss.item()\n",
    " \n",
    "    return running_loss / len(trainloader)\n",
    "\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "\n",
    "def objective(trial):\n",
    "    # Define range of values to be tested for the hyperparameters\n",
    "    num_conv_layers = trial.suggest_int(\"num_conv_layers\", 1, 3)\n",
    "    num_filters = [int(trial.suggest_discrete_uniform(\"num_filter_\" + str(i), 16, 128, 16))\n",
    "                   for i in range(num_conv_layers)]\n",
    "    num_neurons = trial.suggest_int(\"num_neurons\", 10, 400, 10)  # Number of neurons of fully connected layer 1\n",
    "    drop_conv2 = trial.suggest_float(\"drop_conv2\", 0.2, 0.5)     # Dropout for convolutional layer 2\n",
    "    drop_fc1 = trial.suggest_float(\"drop_prob_fc1\", 0.2, 0.5)    # Dropout probability for fully connected layer 1\n",
    "\n",
    "    # Generate the model\n",
    "    model = CNN(trial, num_conv_layers, num_filters, num_neurons, drop_conv2,  drop_fc1)\n",
    "\n",
    "    # Generate the optimizer\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
    "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_model_once(model, criterion, optimizer)\n",
    "        accuracy, test_loss, precision, recall, f1 = test_model(model, criterion)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, \" +\n",
    "              f\"Train Loss: {train_loss:.4f}, \" + \n",
    "              f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "        # Pruning (stops trial early if not promising)\n",
    "        trial.report(accuracy, epoch)\n",
    "        # Handle pruning based on the intermediate value.\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    trial.set_user_attr(\"test_loss\", test_loss)\n",
    "    trial.set_user_attr(\"precision\", precision)\n",
    "    trial.set_user_attr(\"recall\", recall)\n",
    "    trial.set_user_attr(\"f1_score\", f1)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=num_trials)\n",
    "\n",
    "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "print(\"\\n-- Study Statistics --\")\n",
    "print(f\"  Number of finished trials: {len(study.trials)}\")\n",
    "print(f\"  Number of pruned trials:   {len(pruned_trials)}\")\n",
    "print(f\"  Number of complete trials: {len(complete_trials)}\")\n",
    "\n",
    "best_trial = study.best_trial\n",
    "print(\"\\n-- Best Trial --\")\n",
    "print(f\"  Accuracy:  {best_trial.value}\")\n",
    "print(f\"  Test Loss: {best_trial.user_attrs['test_loss']}\")\n",
    "print(f\"  Precision: {best_trial.user_attrs['precision']}\")\n",
    "print(f\"  Recall:    {best_trial.user_attrs['recall']}\")\n",
    "print(f\"  F1 Score:  {best_trial.user_attrs['f1_score']}\")\n",
    "print(\"  Parameters: \")\n",
    "for key, val in best_trial.params.items():\n",
    "    print(f\"    {key}: {(15 - len(key)) * ' '}{val}\")\n",
    "\n",
    "# Save results to csv file\n",
    "df = study.trials_dataframe().drop([\"datetime_start\", \"datetime_complete\", \"duration\"], axis=1)\n",
    "df = df.loc[df[\"state\"] == \"COMPLETE\"]  # Keep only results that did not prune\n",
    "df = df.drop(\"state\", axis=1)\n",
    "df = df.sort_values(\"value\")            # Sort based on accuracy\n",
    "df.to_csv(\"optuna_results.csv\", index=False)\n",
    "\n",
    "print(f\"\\n-- Overall Results (Ordered by Accuracy) --\")\n",
    "print(df)\n",
    "\n",
    "most_important_parameters = optuna.importance.get_param_importances(study, target=None)\n",
    "\n",
    "print(\"\\n-- Most Important Hyperparameters --\")\n",
    "for key, val in most_important_parameters.items():\n",
    "    print(f\"  {key}: {(15 - len(key)) * ' '}{(100 * val):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instânciando o modelo base, definindo parametros, loss function e otimizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaseCNN()\n",
    "\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treinamento o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/1875], Loss: 0.05343368190129598\n",
      "Epoch [1/10], Step [200/1875], Loss: 0.07958543639183044\n",
      "Epoch [1/10], Step [300/1875], Loss: 0.10252286190191905\n",
      "Epoch [1/10], Step [400/1875], Loss: 0.12446597655216853\n",
      "Epoch [1/10], Step [500/1875], Loss: 0.14689539706707\n",
      "Epoch [1/10], Step [600/1875], Loss: 0.16643953625361124\n",
      "Epoch [1/10], Step [700/1875], Loss: 0.18599609969059627\n",
      "Epoch [1/10], Step [800/1875], Loss: 0.20553872314691543\n",
      "Epoch [1/10], Step [900/1875], Loss: 0.2234916055758794\n",
      "Epoch [1/10], Step [1000/1875], Loss: 0.24112669867277145\n",
      "Epoch [1/10], Step [1100/1875], Loss: 0.2591175560017427\n",
      "Epoch [1/10], Step [1200/1875], Loss: 0.27558091139594715\n",
      "Epoch [1/10], Step [1300/1875], Loss: 0.29222207629084584\n",
      "Epoch [1/10], Step [1400/1875], Loss: 0.3091878640671571\n",
      "Epoch [1/10], Step [1500/1875], Loss: 0.3260618733565013\n",
      "Epoch [1/10], Step [1600/1875], Loss: 0.34117929690877596\n",
      "Epoch [1/10], Step [1700/1875], Loss: 0.3563172677616278\n",
      "Epoch [1/10], Step [1800/1875], Loss: 0.3723968612571557\n",
      "Epoch [2/10], Step [100/1875], Loss: 0.015711853619416554\n",
      "Epoch [2/10], Step [200/1875], Loss: 0.030198686397075652\n",
      "Epoch [2/10], Step [300/1875], Loss: 0.04467269413471222\n",
      "Epoch [2/10], Step [400/1875], Loss: 0.05642987917661667\n",
      "Epoch [2/10], Step [500/1875], Loss: 0.06853959084947904\n",
      "Epoch [2/10], Step [600/1875], Loss: 0.08287865369518597\n",
      "Epoch [2/10], Step [700/1875], Loss: 0.09591320491234462\n",
      "Epoch [2/10], Step [800/1875], Loss: 0.10914888218243916\n",
      "Epoch [2/10], Step [900/1875], Loss: 0.12273254745403926\n",
      "Epoch [2/10], Step [1000/1875], Loss: 0.13530065455436707\n",
      "Epoch [2/10], Step [1100/1875], Loss: 0.14785737835566204\n",
      "Epoch [2/10], Step [1200/1875], Loss: 0.1612643791516622\n",
      "Epoch [2/10], Step [1300/1875], Loss: 0.17362617676059405\n",
      "Epoch [2/10], Step [1400/1875], Loss: 0.18709477283159892\n",
      "Epoch [2/10], Step [1500/1875], Loss: 0.1988337099015713\n",
      "Epoch [2/10], Step [1600/1875], Loss: 0.2111862477997939\n",
      "Epoch [2/10], Step [1700/1875], Loss: 0.22304656998415787\n",
      "Epoch [2/10], Step [1800/1875], Loss: 0.2346213516821464\n",
      "Epoch [3/10], Step [100/1875], Loss: 0.010789805382490159\n",
      "Epoch [3/10], Step [200/1875], Loss: 0.021619261238972346\n",
      "Epoch [3/10], Step [300/1875], Loss: 0.0329871318479379\n",
      "Epoch [3/10], Step [400/1875], Loss: 0.04404947997828325\n",
      "Epoch [3/10], Step [500/1875], Loss: 0.056062771955132484\n",
      "Epoch [3/10], Step [600/1875], Loss: 0.06651480148335298\n",
      "Epoch [3/10], Step [700/1875], Loss: 0.07777423396011193\n",
      "Epoch [3/10], Step [800/1875], Loss: 0.0874582148005565\n",
      "Epoch [3/10], Step [900/1875], Loss: 0.09718196173210938\n",
      "Epoch [3/10], Step [1000/1875], Loss: 0.10842270858784517\n",
      "Epoch [3/10], Step [1100/1875], Loss: 0.11919791779816151\n",
      "Epoch [3/10], Step [1200/1875], Loss: 0.12892968453069528\n",
      "Epoch [3/10], Step [1300/1875], Loss: 0.13908705551326275\n",
      "Epoch [3/10], Step [1400/1875], Loss: 0.14865998948216438\n",
      "Epoch [3/10], Step [1500/1875], Loss: 0.15858013353149097\n",
      "Epoch [3/10], Step [1600/1875], Loss: 0.16899899662137033\n",
      "Epoch [3/10], Step [1700/1875], Loss: 0.1792129600505034\n",
      "Epoch [3/10], Step [1800/1875], Loss: 0.1894238532861074\n",
      "Epoch [4/10], Step [100/1875], Loss: 0.008749606397747994\n",
      "Epoch [4/10], Step [200/1875], Loss: 0.01753558172484239\n",
      "Epoch [4/10], Step [300/1875], Loss: 0.026019032981991767\n",
      "Epoch [4/10], Step [400/1875], Loss: 0.03537838434080283\n",
      "Epoch [4/10], Step [500/1875], Loss: 0.04502302139699459\n",
      "Epoch [4/10], Step [600/1875], Loss: 0.05458357737461726\n",
      "Epoch [4/10], Step [700/1875], Loss: 0.06450637766619523\n",
      "Epoch [4/10], Step [800/1875], Loss: 0.07437535293797652\n",
      "Epoch [4/10], Step [900/1875], Loss: 0.08296770752370357\n",
      "Epoch [4/10], Step [1000/1875], Loss: 0.09173232608834903\n",
      "Epoch [4/10], Step [1100/1875], Loss: 0.09999950358668963\n",
      "Epoch [4/10], Step [1200/1875], Loss: 0.10894137565692266\n",
      "Epoch [4/10], Step [1300/1875], Loss: 0.11818209666411082\n",
      "Epoch [4/10], Step [1400/1875], Loss: 0.12739696514606474\n",
      "Epoch [4/10], Step [1500/1875], Loss: 0.13637061386207738\n",
      "Epoch [4/10], Step [1600/1875], Loss: 0.14473401786784332\n",
      "Epoch [4/10], Step [1700/1875], Loss: 0.15319680750270684\n",
      "Epoch [4/10], Step [1800/1875], Loss: 0.16131938312351704\n",
      "Epoch [5/10], Step [100/1875], Loss: 0.007764801440636317\n",
      "Epoch [5/10], Step [200/1875], Loss: 0.01571045343875885\n",
      "Epoch [5/10], Step [300/1875], Loss: 0.023799903105696043\n",
      "Epoch [5/10], Step [400/1875], Loss: 0.031351177003979686\n",
      "Epoch [5/10], Step [500/1875], Loss: 0.03896467111806075\n",
      "Epoch [5/10], Step [600/1875], Loss: 0.04721187200943629\n",
      "Epoch [5/10], Step [700/1875], Loss: 0.05543342833518982\n",
      "Epoch [5/10], Step [800/1875], Loss: 0.06385690338512262\n",
      "Epoch [5/10], Step [900/1875], Loss: 0.07216224708755811\n",
      "Epoch [5/10], Step [1000/1875], Loss: 0.08006538177331289\n",
      "Epoch [5/10], Step [1100/1875], Loss: 0.08936295804679394\n",
      "Epoch [5/10], Step [1200/1875], Loss: 0.09696734841763974\n",
      "Epoch [5/10], Step [1300/1875], Loss: 0.1049485888376832\n",
      "Epoch [5/10], Step [1400/1875], Loss: 0.11310696760465701\n",
      "Epoch [5/10], Step [1500/1875], Loss: 0.12195140723834436\n",
      "Epoch [5/10], Step [1600/1875], Loss: 0.13084317409843207\n",
      "Epoch [5/10], Step [1700/1875], Loss: 0.13885819255659979\n",
      "Epoch [5/10], Step [1800/1875], Loss: 0.1472181852226456\n",
      "Epoch [6/10], Step [100/1875], Loss: 0.0068453269551197685\n",
      "Epoch [6/10], Step [200/1875], Loss: 0.014416585699717203\n",
      "Epoch [6/10], Step [300/1875], Loss: 0.02183963019301494\n",
      "Epoch [6/10], Step [400/1875], Loss: 0.02902133064866066\n",
      "Epoch [6/10], Step [500/1875], Loss: 0.03650233122507731\n",
      "Epoch [6/10], Step [600/1875], Loss: 0.04371601880937815\n",
      "Epoch [6/10], Step [700/1875], Loss: 0.051799867172042525\n",
      "Epoch [6/10], Step [800/1875], Loss: 0.05898469614883264\n",
      "Epoch [6/10], Step [900/1875], Loss: 0.06682063328425089\n",
      "Epoch [6/10], Step [1000/1875], Loss: 0.0741928216745456\n",
      "Epoch [6/10], Step [1100/1875], Loss: 0.08124189390738805\n",
      "Epoch [6/10], Step [1200/1875], Loss: 0.0886575133005778\n",
      "Epoch [6/10], Step [1300/1875], Loss: 0.09668893719812234\n",
      "Epoch [6/10], Step [1400/1875], Loss: 0.1034069286108017\n",
      "Epoch [6/10], Step [1500/1875], Loss: 0.11103449242860079\n",
      "Epoch [6/10], Step [1600/1875], Loss: 0.11815177664111058\n",
      "Epoch [6/10], Step [1700/1875], Loss: 0.12544588154107333\n",
      "Epoch [6/10], Step [1800/1875], Loss: 0.13235956049611172\n",
      "Epoch [7/10], Step [100/1875], Loss: 0.006664735040565332\n",
      "Epoch [7/10], Step [200/1875], Loss: 0.0144441715794305\n",
      "Epoch [7/10], Step [300/1875], Loss: 0.02210453068589171\n",
      "Epoch [7/10], Step [400/1875], Loss: 0.02945098845983545\n",
      "Epoch [7/10], Step [500/1875], Loss: 0.03579033226196965\n",
      "Epoch [7/10], Step [600/1875], Loss: 0.04251909535353383\n",
      "Epoch [7/10], Step [700/1875], Loss: 0.04959867780134082\n",
      "Epoch [7/10], Step [800/1875], Loss: 0.05679684371451537\n",
      "Epoch [7/10], Step [900/1875], Loss: 0.06311001279676955\n",
      "Epoch [7/10], Step [1000/1875], Loss: 0.07126616175497572\n",
      "Epoch [7/10], Step [1100/1875], Loss: 0.07716327873443564\n",
      "Epoch [7/10], Step [1200/1875], Loss: 0.08454025620445609\n",
      "Epoch [7/10], Step [1300/1875], Loss: 0.0920164952389896\n",
      "Epoch [7/10], Step [1400/1875], Loss: 0.09844188305512071\n",
      "Epoch [7/10], Step [1500/1875], Loss: 0.10531735355332494\n",
      "Epoch [7/10], Step [1600/1875], Loss: 0.11256690258855621\n",
      "Epoch [7/10], Step [1700/1875], Loss: 0.11926005948310098\n",
      "Epoch [7/10], Step [1800/1875], Loss: 0.12611777809585134\n",
      "Epoch [8/10], Step [100/1875], Loss: 0.005761838334798813\n",
      "Epoch [8/10], Step [200/1875], Loss: 0.012454894815385342\n",
      "Epoch [8/10], Step [300/1875], Loss: 0.019745802334447702\n",
      "Epoch [8/10], Step [400/1875], Loss: 0.02591203842709462\n",
      "Epoch [8/10], Step [500/1875], Loss: 0.0334560211862127\n",
      "Epoch [8/10], Step [600/1875], Loss: 0.03999876669148604\n",
      "Epoch [8/10], Step [700/1875], Loss: 0.0468052634631594\n",
      "Epoch [8/10], Step [800/1875], Loss: 0.05263708033512036\n",
      "Epoch [8/10], Step [900/1875], Loss: 0.05925204350004593\n",
      "Epoch [8/10], Step [1000/1875], Loss: 0.06668971060862144\n",
      "Epoch [8/10], Step [1100/1875], Loss: 0.07301378784924746\n",
      "Epoch [8/10], Step [1200/1875], Loss: 0.08036737843652567\n",
      "Epoch [8/10], Step [1300/1875], Loss: 0.08642321731746197\n",
      "Epoch [8/10], Step [1400/1875], Loss: 0.09297783907850583\n",
      "Epoch [8/10], Step [1500/1875], Loss: 0.09958302985628446\n",
      "Epoch [8/10], Step [1600/1875], Loss: 0.10694375211795171\n",
      "Epoch [8/10], Step [1700/1875], Loss: 0.11266614352961381\n",
      "Epoch [8/10], Step [1800/1875], Loss: 0.11937842405438423\n",
      "Epoch [9/10], Step [100/1875], Loss: 0.006207255864640077\n",
      "Epoch [9/10], Step [200/1875], Loss: 0.011615692121287187\n",
      "Epoch [9/10], Step [300/1875], Loss: 0.01743809700856606\n",
      "Epoch [9/10], Step [400/1875], Loss: 0.023339026818672816\n",
      "Epoch [9/10], Step [500/1875], Loss: 0.02999959867099921\n",
      "Epoch [9/10], Step [600/1875], Loss: 0.036136814541121325\n",
      "Epoch [9/10], Step [700/1875], Loss: 0.04158662752211094\n",
      "Epoch [9/10], Step [800/1875], Loss: 0.04787236565599839\n",
      "Epoch [9/10], Step [900/1875], Loss: 0.053852728446821374\n",
      "Epoch [9/10], Step [1000/1875], Loss: 0.060466782450675965\n",
      "Epoch [9/10], Step [1100/1875], Loss: 0.06684153763999541\n",
      "Epoch [9/10], Step [1200/1875], Loss: 0.07273555059432983\n",
      "Epoch [9/10], Step [1300/1875], Loss: 0.0787257171312968\n",
      "Epoch [9/10], Step [1400/1875], Loss: 0.08456265939498941\n",
      "Epoch [9/10], Step [1500/1875], Loss: 0.09112913598492742\n",
      "Epoch [9/10], Step [1600/1875], Loss: 0.09872992729693651\n",
      "Epoch [9/10], Step [1700/1875], Loss: 0.10458272509674231\n",
      "Epoch [9/10], Step [1800/1875], Loss: 0.11070452547222376\n",
      "Epoch [10/10], Step [100/1875], Loss: 0.005369751646618048\n",
      "Epoch [10/10], Step [200/1875], Loss: 0.010898681453615427\n",
      "Epoch [10/10], Step [300/1875], Loss: 0.016317271783699593\n",
      "Epoch [10/10], Step [400/1875], Loss: 0.022484939476599297\n",
      "Epoch [10/10], Step [500/1875], Loss: 0.02971760944351554\n",
      "Epoch [10/10], Step [600/1875], Loss: 0.03521013264283538\n",
      "Epoch [10/10], Step [700/1875], Loss: 0.04158024301255743\n",
      "Epoch [10/10], Step [800/1875], Loss: 0.04683405673677723\n",
      "Epoch [10/10], Step [900/1875], Loss: 0.05202161181842287\n",
      "Epoch [10/10], Step [1000/1875], Loss: 0.05838663737947742\n",
      "Epoch [10/10], Step [1100/1875], Loss: 0.06422653246447443\n",
      "Epoch [10/10], Step [1200/1875], Loss: 0.07064397783949972\n",
      "Epoch [10/10], Step [1300/1875], Loss: 0.07599226324558259\n",
      "Epoch [10/10], Step [1400/1875], Loss: 0.08082488616208236\n",
      "Epoch [10/10], Step [1500/1875], Loss: 0.0861163060704867\n",
      "Epoch [10/10], Step [1600/1875], Loss: 0.09236018198082845\n",
      "Epoch [10/10], Step [1700/1875], Loss: 0.09812404111574094\n",
      "Epoch [10/10], Step [1800/1875], Loss: 0.10460521846910317\n",
      "Finished Training!\n"
     ]
    }
   ],
   "source": [
    "train_model(model, criterion, optimizer, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testando o modleo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.49%, Test Loss: 0.0824, Precision: 0.9749, Recall: 0.9749, F1 Score: 0.9749\n"
     ]
    }
   ],
   "source": [
    "accuracy, loss, precision, recall, f1 = test_model(model, criterion)\n",
    "print(f'Accuracy: {100 * accuracy}%, Test Loss: {loss:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvando o modelo treinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('modelos_treinados'):\n",
    "    os.makedirs('modelos_treinados')\n",
    "\n",
    "current_datetime = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "path_model_trained = os.path.join('modelos_treinados', f'model_{current_datetime}.pth')\n",
    "\n",
    "torch.save(model.state_dict(), path_model_trained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Função de criação de derivação da rede base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn(conv_kernel_size, conv_stride, conv_padding, pool_kernel_size, pool_stride, dropout_prob):\n",
    "    class CNNTest(BaseCNN):\n",
    "        def __init__(self):\n",
    "            super(CNNTest, self).__init__(conv_kernel_size, conv_stride, conv_padding, pool_kernel_size, pool_stride, dropout_prob)\n",
    "    return CNNTest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instanciando os modelos com grid de parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = [\n",
    "    {'conv_kernel_size': 3, 'conv_stride': 1, 'conv_padding': 1, 'pool_kernel_size': 2, 'pool_stride': 2, 'dropout_prob': 0.5},\n",
    "    {'conv_kernel_size': 5, 'conv_stride': 1, 'conv_padding': 2, 'pool_kernel_size': 2, 'pool_stride': 2, 'dropout_prob': 0.3},\n",
    "    {'conv_kernel_size': 3, 'conv_stride': 1, 'conv_padding': 1, 'pool_kernel_size': 2, 'pool_stride': 2, 'dropout_prob': 0.7}\n",
    "]\n",
    "networks = [create_cnn(**params) for params in grid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definindo parametros adicionais, loss function e otimizadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizers = [optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9) for model in networks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treinando e testando os modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinando Rede Neural 1 com os seguintes parâmetros:\n",
      "{'conv_kernel_size': 3, 'conv_stride': 1, 'conv_padding': 1, 'pool_kernel_size': 2, 'pool_stride': 2, 'dropout_prob': 0.5}\n",
      "Epoch [1/10], Step [100/1875], Loss: 0.054824722655614215\n",
      "Epoch [1/10], Step [200/1875], Loss: 0.0823668920914332\n",
      "Epoch [1/10], Step [300/1875], Loss: 0.1052058957417806\n",
      "Epoch [1/10], Step [400/1875], Loss: 0.12593787559270858\n",
      "Epoch [1/10], Step [500/1875], Loss: 0.14415096456607182\n",
      "Epoch [1/10], Step [600/1875], Loss: 0.16427258375088374\n",
      "Epoch [1/10], Step [700/1875], Loss: 0.18417421905795733\n",
      "Epoch [1/10], Step [800/1875], Loss: 0.20295052731633187\n",
      "Epoch [1/10], Step [900/1875], Loss: 0.2213740981042385\n",
      "Epoch [1/10], Step [1000/1875], Loss: 0.23889432164231936\n",
      "Epoch [1/10], Step [1100/1875], Loss: 0.25756438636183737\n",
      "Epoch [1/10], Step [1200/1875], Loss: 0.274110874102513\n",
      "Epoch [1/10], Step [1300/1875], Loss: 0.29046420483986535\n",
      "Epoch [1/10], Step [1400/1875], Loss: 0.3069451700607936\n",
      "Epoch [1/10], Step [1500/1875], Loss: 0.322396982584397\n",
      "Epoch [1/10], Step [1600/1875], Loss: 0.33649732356468837\n",
      "Epoch [1/10], Step [1700/1875], Loss: 0.35091231865088146\n",
      "Epoch [1/10], Step [1800/1875], Loss: 0.3656045854171117\n",
      "Epoch [2/10], Step [100/1875], Loss: 0.01314278953075409\n",
      "Epoch [2/10], Step [200/1875], Loss: 0.026428262265523274\n",
      "Epoch [2/10], Step [300/1875], Loss: 0.04101164694627126\n",
      "Epoch [2/10], Step [400/1875], Loss: 0.05464415280818939\n",
      "Epoch [2/10], Step [500/1875], Loss: 0.06748474684854348\n",
      "Epoch [2/10], Step [600/1875], Loss: 0.08111636588275432\n",
      "Epoch [2/10], Step [700/1875], Loss: 0.09484552385707697\n",
      "Epoch [2/10], Step [800/1875], Loss: 0.10765300791760286\n",
      "Epoch [2/10], Step [900/1875], Loss: 0.12049617678026359\n",
      "Epoch [2/10], Step [1000/1875], Loss: 0.1333702140023311\n",
      "Epoch [2/10], Step [1100/1875], Loss: 0.14580261885424456\n",
      "Epoch [2/10], Step [1200/1875], Loss: 0.15761414168179036\n",
      "Epoch [2/10], Step [1300/1875], Loss: 0.17056312044163546\n",
      "Epoch [2/10], Step [1400/1875], Loss: 0.1832339676429828\n",
      "Epoch [2/10], Step [1500/1875], Loss: 0.19500731062392393\n",
      "Epoch [2/10], Step [1600/1875], Loss: 0.20664627677301567\n",
      "Epoch [2/10], Step [1700/1875], Loss: 0.21685116577843824\n",
      "Epoch [2/10], Step [1800/1875], Loss: 0.22739749897221725\n",
      "Epoch [3/10], Step [100/1875], Loss: 0.010693706647555033\n",
      "Epoch [3/10], Step [200/1875], Loss: 0.022056854502360026\n",
      "Epoch [3/10], Step [300/1875], Loss: 0.03281431712210178\n",
      "Epoch [3/10], Step [400/1875], Loss: 0.043298493380347886\n",
      "Epoch [3/10], Step [500/1875], Loss: 0.05371542227466901\n",
      "Epoch [3/10], Step [600/1875], Loss: 0.06444464854697386\n",
      "Epoch [3/10], Step [700/1875], Loss: 0.07535618683795134\n",
      "Epoch [3/10], Step [800/1875], Loss: 0.08586526954074701\n",
      "Epoch [3/10], Step [900/1875], Loss: 0.09614218754271667\n",
      "Epoch [3/10], Step [1000/1875], Loss: 0.10644623793462912\n",
      "Epoch [3/10], Step [1100/1875], Loss: 0.1160798115024964\n",
      "Epoch [3/10], Step [1200/1875], Loss: 0.12694521604875725\n",
      "Epoch [3/10], Step [1300/1875], Loss: 0.1373778468320767\n",
      "Epoch [3/10], Step [1400/1875], Loss: 0.1469656039694945\n",
      "Epoch [3/10], Step [1500/1875], Loss: 0.15760872590839864\n",
      "Epoch [3/10], Step [1600/1875], Loss: 0.16738390222787858\n",
      "Epoch [3/10], Step [1700/1875], Loss: 0.17695858779946963\n",
      "Epoch [3/10], Step [1800/1875], Loss: 0.18624491702417534\n",
      "Epoch [4/10], Step [100/1875], Loss: 0.008318516009052595\n",
      "Epoch [4/10], Step [200/1875], Loss: 0.01692388872007529\n",
      "Epoch [4/10], Step [300/1875], Loss: 0.026435066126783688\n",
      "Epoch [4/10], Step [400/1875], Loss: 0.03580382979214192\n",
      "Epoch [4/10], Step [500/1875], Loss: 0.04543623769978682\n",
      "Epoch [4/10], Step [600/1875], Loss: 0.05444101420044899\n",
      "Epoch [4/10], Step [700/1875], Loss: 0.06399257540603479\n",
      "Epoch [4/10], Step [800/1875], Loss: 0.07382127160429955\n",
      "Epoch [4/10], Step [900/1875], Loss: 0.08289955660601457\n",
      "Epoch [4/10], Step [1000/1875], Loss: 0.09182642869949341\n",
      "Epoch [4/10], Step [1100/1875], Loss: 0.10004131541748841\n",
      "Epoch [4/10], Step [1200/1875], Loss: 0.10883728866378466\n",
      "Epoch [4/10], Step [1300/1875], Loss: 0.11859123526513576\n",
      "Epoch [4/10], Step [1400/1875], Loss: 0.12764825180321931\n",
      "Epoch [4/10], Step [1500/1875], Loss: 0.13727235215852657\n",
      "Epoch [4/10], Step [1600/1875], Loss: 0.14543780460208655\n",
      "Epoch [4/10], Step [1700/1875], Loss: 0.15383733335485061\n",
      "Epoch [4/10], Step [1800/1875], Loss: 0.1624620034173131\n",
      "Epoch [5/10], Step [100/1875], Loss: 0.00836557236413161\n",
      "Epoch [5/10], Step [200/1875], Loss: 0.016072734108567237\n",
      "Epoch [5/10], Step [300/1875], Loss: 0.024954413716991742\n",
      "Epoch [5/10], Step [400/1875], Loss: 0.032113670978943507\n",
      "Epoch [5/10], Step [500/1875], Loss: 0.040776262765129405\n",
      "Epoch [5/10], Step [600/1875], Loss: 0.04936447415749232\n",
      "Epoch [5/10], Step [700/1875], Loss: 0.0570034606685241\n",
      "Epoch [5/10], Step [800/1875], Loss: 0.06479125750760237\n",
      "Epoch [5/10], Step [900/1875], Loss: 0.07243874060213566\n",
      "Epoch [5/10], Step [1000/1875], Loss: 0.08082445255915324\n",
      "Epoch [5/10], Step [1100/1875], Loss: 0.0889744712740183\n",
      "Epoch [5/10], Step [1200/1875], Loss: 0.09679159137109915\n",
      "Epoch [5/10], Step [1300/1875], Loss: 0.10490596678505341\n",
      "Epoch [5/10], Step [1400/1875], Loss: 0.11322106413592894\n",
      "Epoch [5/10], Step [1500/1875], Loss: 0.12175769692609707\n",
      "Epoch [5/10], Step [1600/1875], Loss: 0.1300782442326347\n",
      "Epoch [5/10], Step [1700/1875], Loss: 0.1380415520315369\n",
      "Epoch [5/10], Step [1800/1875], Loss: 0.14568695198049147\n",
      "Epoch [6/10], Step [100/1875], Loss: 0.007585224297642708\n",
      "Epoch [6/10], Step [200/1875], Loss: 0.015021649908026059\n",
      "Epoch [6/10], Step [300/1875], Loss: 0.0230359697903196\n",
      "Epoch [6/10], Step [400/1875], Loss: 0.03067098836650451\n",
      "Epoch [6/10], Step [500/1875], Loss: 0.037925481929878394\n",
      "Epoch [6/10], Step [600/1875], Loss: 0.04466709793955088\n",
      "Epoch [6/10], Step [700/1875], Loss: 0.051692949420710406\n",
      "Epoch [6/10], Step [800/1875], Loss: 0.05871503741989533\n",
      "Epoch [6/10], Step [900/1875], Loss: 0.06606922216763099\n",
      "Epoch [6/10], Step [1000/1875], Loss: 0.0734072246576349\n",
      "Epoch [6/10], Step [1100/1875], Loss: 0.08037952333837747\n",
      "Epoch [6/10], Step [1200/1875], Loss: 0.08822703711787859\n",
      "Epoch [6/10], Step [1300/1875], Loss: 0.09618668389568726\n",
      "Epoch [6/10], Step [1400/1875], Loss: 0.10303894703388214\n",
      "Epoch [6/10], Step [1500/1875], Loss: 0.11043647309044997\n",
      "Epoch [6/10], Step [1600/1875], Loss: 0.11865898642937342\n",
      "Epoch [6/10], Step [1700/1875], Loss: 0.12618077387213708\n",
      "Epoch [6/10], Step [1800/1875], Loss: 0.13399688437730073\n",
      "Epoch [7/10], Step [100/1875], Loss: 0.007068901638189951\n",
      "Epoch [7/10], Step [200/1875], Loss: 0.014088643588870764\n",
      "Epoch [7/10], Step [300/1875], Loss: 0.020219327645748852\n",
      "Epoch [7/10], Step [400/1875], Loss: 0.02704645958219965\n",
      "Epoch [7/10], Step [500/1875], Loss: 0.034230613263199726\n",
      "Epoch [7/10], Step [600/1875], Loss: 0.04084496647790074\n",
      "Epoch [7/10], Step [700/1875], Loss: 0.0485899642534554\n",
      "Epoch [7/10], Step [800/1875], Loss: 0.05563127092892925\n",
      "Epoch [7/10], Step [900/1875], Loss: 0.06196337332452337\n",
      "Epoch [7/10], Step [1000/1875], Loss: 0.06928862612918019\n",
      "Epoch [7/10], Step [1100/1875], Loss: 0.07706946714793643\n",
      "Epoch [7/10], Step [1200/1875], Loss: 0.08385724374130368\n",
      "Epoch [7/10], Step [1300/1875], Loss: 0.08972741121674577\n",
      "Epoch [7/10], Step [1400/1875], Loss: 0.09623454650218288\n",
      "Epoch [7/10], Step [1500/1875], Loss: 0.10370633257900676\n",
      "Epoch [7/10], Step [1600/1875], Loss: 0.11083509273057182\n",
      "Epoch [7/10], Step [1700/1875], Loss: 0.11729215651129683\n",
      "Epoch [7/10], Step [1800/1875], Loss: 0.12325154442464312\n",
      "Epoch [8/10], Step [100/1875], Loss: 0.005976324233412743\n",
      "Epoch [8/10], Step [200/1875], Loss: 0.012314014103760321\n",
      "Epoch [8/10], Step [300/1875], Loss: 0.019476114393025636\n",
      "Epoch [8/10], Step [400/1875], Loss: 0.025887711975475153\n",
      "Epoch [8/10], Step [500/1875], Loss: 0.03291603624423345\n",
      "Epoch [8/10], Step [600/1875], Loss: 0.0396713039641579\n",
      "Epoch [8/10], Step [700/1875], Loss: 0.04658181784401337\n",
      "Epoch [8/10], Step [800/1875], Loss: 0.052902592900892095\n",
      "Epoch [8/10], Step [900/1875], Loss: 0.058925905672709146\n",
      "Epoch [8/10], Step [1000/1875], Loss: 0.0654558102329572\n",
      "Epoch [8/10], Step [1100/1875], Loss: 0.07137339317550262\n",
      "Epoch [8/10], Step [1200/1875], Loss: 0.07790372075885535\n",
      "Epoch [8/10], Step [1300/1875], Loss: 0.08380260055114826\n",
      "Epoch [8/10], Step [1400/1875], Loss: 0.09035076679090659\n",
      "Epoch [8/10], Step [1500/1875], Loss: 0.09616733771363895\n",
      "Epoch [8/10], Step [1600/1875], Loss: 0.10325321895132462\n",
      "Epoch [8/10], Step [1700/1875], Loss: 0.10974236067732175\n",
      "Epoch [8/10], Step [1800/1875], Loss: 0.115775697016716\n",
      "Epoch [9/10], Step [100/1875], Loss: 0.005935973877211412\n",
      "Epoch [9/10], Step [200/1875], Loss: 0.01269036623686552\n",
      "Epoch [9/10], Step [300/1875], Loss: 0.019171309810380142\n",
      "Epoch [9/10], Step [400/1875], Loss: 0.025953978692491848\n",
      "Epoch [9/10], Step [500/1875], Loss: 0.03209398557990789\n",
      "Epoch [9/10], Step [600/1875], Loss: 0.037451338095466295\n",
      "Epoch [9/10], Step [700/1875], Loss: 0.04494740314980348\n",
      "Epoch [9/10], Step [800/1875], Loss: 0.05121928938825925\n",
      "Epoch [9/10], Step [900/1875], Loss: 0.05770875691945354\n",
      "Epoch [9/10], Step [1000/1875], Loss: 0.06403114484076698\n",
      "Epoch [9/10], Step [1100/1875], Loss: 0.06983669577017426\n",
      "Epoch [9/10], Step [1200/1875], Loss: 0.07604415568386515\n",
      "Epoch [9/10], Step [1300/1875], Loss: 0.08254486367230614\n",
      "Epoch [9/10], Step [1400/1875], Loss: 0.08858262834325432\n",
      "Epoch [9/10], Step [1500/1875], Loss: 0.09402488366017739\n",
      "Epoch [9/10], Step [1600/1875], Loss: 0.10045252339591583\n",
      "Epoch [9/10], Step [1700/1875], Loss: 0.10648401145935059\n",
      "Epoch [9/10], Step [1800/1875], Loss: 0.11170790203362703\n",
      "Epoch [10/10], Step [100/1875], Loss: 0.006259655185540517\n",
      "Epoch [10/10], Step [200/1875], Loss: 0.01288184604048729\n",
      "Epoch [10/10], Step [300/1875], Loss: 0.01855169211924076\n",
      "Epoch [10/10], Step [400/1875], Loss: 0.024223724709451198\n",
      "Epoch [10/10], Step [500/1875], Loss: 0.029920263094206652\n",
      "Epoch [10/10], Step [600/1875], Loss: 0.03681437540402015\n",
      "Epoch [10/10], Step [700/1875], Loss: 0.042459446488320826\n",
      "Epoch [10/10], Step [800/1875], Loss: 0.048211239134768644\n",
      "Epoch [10/10], Step [900/1875], Loss: 0.054599719497561455\n",
      "Epoch [10/10], Step [1000/1875], Loss: 0.05994350552161535\n",
      "Epoch [10/10], Step [1100/1875], Loss: 0.06579067618896564\n",
      "Epoch [10/10], Step [1200/1875], Loss: 0.07178668200597167\n",
      "Epoch [10/10], Step [1300/1875], Loss: 0.07734834630712867\n",
      "Epoch [10/10], Step [1400/1875], Loss: 0.08264193342849613\n",
      "Epoch [10/10], Step [1500/1875], Loss: 0.08945738600318631\n",
      "Epoch [10/10], Step [1600/1875], Loss: 0.09551632657696803\n",
      "Epoch [10/10], Step [1700/1875], Loss: 0.10128873481874665\n",
      "Epoch [10/10], Step [1800/1875], Loss: 0.10757707276269793\n",
      "Finished Training!\n",
      "Accuracy: 97.52% Test Loss: 0.0802 Precision: 0.9753 Recall: 0.9752 F1 Score: 0.9752 Fit Time: 4679.9\n",
      "------------------------------------\n",
      "\n",
      "Treinando Rede Neural 2 com os seguintes parâmetros:\n",
      "{'conv_kernel_size': 5, 'conv_stride': 1, 'conv_padding': 2, 'pool_kernel_size': 2, 'pool_stride': 2, 'dropout_prob': 0.3}\n",
      "Epoch [1/10], Step [100/1875], Loss: 0.05221187845865886\n",
      "Epoch [1/10], Step [200/1875], Loss: 0.07846618865331013\n",
      "Epoch [1/10], Step [300/1875], Loss: 0.09926770722866059\n",
      "Epoch [1/10], Step [400/1875], Loss: 0.12149273226658504\n",
      "Epoch [1/10], Step [500/1875], Loss: 0.14050867905219397\n",
      "Epoch [1/10], Step [600/1875], Loss: 0.15858971108198167\n",
      "Epoch [1/10], Step [700/1875], Loss: 0.17641694335540137\n",
      "Epoch [1/10], Step [800/1875], Loss: 0.19397429454326628\n",
      "Epoch [1/10], Step [900/1875], Loss: 0.20976540657877923\n",
      "Epoch [1/10], Step [1000/1875], Loss: 0.22566702011227607\n",
      "Epoch [1/10], Step [1100/1875], Loss: 0.24167613028089205\n",
      "Epoch [1/10], Step [1200/1875], Loss: 0.25713452850381535\n",
      "Epoch [1/10], Step [1300/1875], Loss: 0.27181591909130415\n",
      "Epoch [1/10], Step [1400/1875], Loss: 0.2868804927448432\n",
      "Epoch [1/10], Step [1500/1875], Loss: 0.300873752139012\n",
      "Epoch [1/10], Step [1600/1875], Loss: 0.3153983113586903\n",
      "Epoch [1/10], Step [1700/1875], Loss: 0.3290631934026877\n",
      "Epoch [1/10], Step [1800/1875], Loss: 0.34278561329046886\n",
      "Epoch [2/10], Step [100/1875], Loss: 0.01201565250356992\n",
      "Epoch [2/10], Step [200/1875], Loss: 0.02439998623728752\n",
      "Epoch [2/10], Step [300/1875], Loss: 0.03654897105892499\n",
      "Epoch [2/10], Step [400/1875], Loss: 0.047817219897111254\n",
      "Epoch [2/10], Step [500/1875], Loss: 0.058972723988691966\n",
      "Epoch [2/10], Step [600/1875], Loss: 0.06988273891806603\n",
      "Epoch [2/10], Step [700/1875], Loss: 0.0811229575574398\n",
      "Epoch [2/10], Step [800/1875], Loss: 0.0926036415775617\n",
      "Epoch [2/10], Step [900/1875], Loss: 0.10403692739804586\n",
      "Epoch [2/10], Step [1000/1875], Loss: 0.11513334532976151\n",
      "Epoch [2/10], Step [1100/1875], Loss: 0.1253029644747575\n",
      "Epoch [2/10], Step [1200/1875], Loss: 0.1360883358180523\n",
      "Epoch [2/10], Step [1300/1875], Loss: 0.14539571187297504\n",
      "Epoch [2/10], Step [1400/1875], Loss: 0.15547931655744712\n",
      "Epoch [2/10], Step [1500/1875], Loss: 0.16602954873144626\n",
      "Epoch [2/10], Step [1600/1875], Loss: 0.1757981768776973\n",
      "Epoch [2/10], Step [1700/1875], Loss: 0.18457121640841165\n",
      "Epoch [2/10], Step [1800/1875], Loss: 0.19396320276657741\n",
      "Epoch [3/10], Step [100/1875], Loss: 0.008199551592270534\n",
      "Epoch [3/10], Step [200/1875], Loss: 0.01672097291350365\n",
      "Epoch [3/10], Step [300/1875], Loss: 0.025098482926686604\n",
      "Epoch [3/10], Step [400/1875], Loss: 0.034903348211447395\n",
      "Epoch [3/10], Step [500/1875], Loss: 0.04301467642088731\n",
      "Epoch [3/10], Step [600/1875], Loss: 0.051356170812249184\n",
      "Epoch [3/10], Step [700/1875], Loss: 0.05859695129692555\n",
      "Epoch [3/10], Step [800/1875], Loss: 0.06581215013861656\n",
      "Epoch [3/10], Step [900/1875], Loss: 0.07433891260325909\n",
      "Epoch [3/10], Step [1000/1875], Loss: 0.08198738261361917\n",
      "Epoch [3/10], Step [1100/1875], Loss: 0.09081936329305172\n",
      "Epoch [3/10], Step [1200/1875], Loss: 0.0988381972203652\n",
      "Epoch [3/10], Step [1300/1875], Loss: 0.10722427254766226\n",
      "Epoch [3/10], Step [1400/1875], Loss: 0.11479634987662236\n",
      "Epoch [3/10], Step [1500/1875], Loss: 0.12250591867417097\n",
      "Epoch [3/10], Step [1600/1875], Loss: 0.130320450433592\n",
      "Epoch [3/10], Step [1700/1875], Loss: 0.13785796828816335\n",
      "Epoch [3/10], Step [1800/1875], Loss: 0.1452566645508011\n",
      "Epoch [4/10], Step [100/1875], Loss: 0.006237143118182818\n",
      "Epoch [4/10], Step [200/1875], Loss: 0.012844258066515127\n",
      "Epoch [4/10], Step [300/1875], Loss: 0.02002933672318856\n",
      "Epoch [4/10], Step [400/1875], Loss: 0.026562210229535897\n",
      "Epoch [4/10], Step [500/1875], Loss: 0.0329819178784887\n",
      "Epoch [4/10], Step [600/1875], Loss: 0.041054805312554045\n",
      "Epoch [4/10], Step [700/1875], Loss: 0.04800681431591511\n",
      "Epoch [4/10], Step [800/1875], Loss: 0.054239906696478525\n",
      "Epoch [4/10], Step [900/1875], Loss: 0.06100657793680827\n",
      "Epoch [4/10], Step [1000/1875], Loss: 0.06797525051583847\n",
      "Epoch [4/10], Step [1100/1875], Loss: 0.07483408566216627\n",
      "Epoch [4/10], Step [1200/1875], Loss: 0.08118711346834898\n",
      "Epoch [4/10], Step [1300/1875], Loss: 0.0879268474633495\n",
      "Epoch [4/10], Step [1400/1875], Loss: 0.09375383348117272\n",
      "Epoch [4/10], Step [1500/1875], Loss: 0.10109925552954277\n",
      "Epoch [4/10], Step [1600/1875], Loss: 0.10781417954315742\n",
      "Epoch [4/10], Step [1700/1875], Loss: 0.11392807675550382\n",
      "Epoch [4/10], Step [1800/1875], Loss: 0.12076851141800483\n",
      "Epoch [5/10], Step [100/1875], Loss: 0.005878714297711849\n",
      "Epoch [5/10], Step [200/1875], Loss: 0.01191624696701765\n",
      "Epoch [5/10], Step [300/1875], Loss: 0.01859878121862809\n",
      "Epoch [5/10], Step [400/1875], Loss: 0.02432354693636298\n",
      "Epoch [5/10], Step [500/1875], Loss: 0.030552359452595313\n",
      "Epoch [5/10], Step [600/1875], Loss: 0.03644277935201923\n",
      "Epoch [5/10], Step [700/1875], Loss: 0.041967075518518686\n",
      "Epoch [5/10], Step [800/1875], Loss: 0.04812481423541903\n",
      "Epoch [5/10], Step [900/1875], Loss: 0.05338902962729335\n",
      "Epoch [5/10], Step [1000/1875], Loss: 0.05935935011034211\n",
      "Epoch [5/10], Step [1100/1875], Loss: 0.06480184276625514\n",
      "Epoch [5/10], Step [1200/1875], Loss: 0.070506408534199\n",
      "Epoch [5/10], Step [1300/1875], Loss: 0.07621378576333324\n",
      "Epoch [5/10], Step [1400/1875], Loss: 0.08255350495055318\n",
      "Epoch [5/10], Step [1500/1875], Loss: 0.08802657212863366\n",
      "Epoch [5/10], Step [1600/1875], Loss: 0.09396027396917343\n",
      "Epoch [5/10], Step [1700/1875], Loss: 0.09980273409585158\n",
      "Epoch [5/10], Step [1800/1875], Loss: 0.10504138508240382\n",
      "Epoch [6/10], Step [100/1875], Loss: 0.0047819305643439294\n",
      "Epoch [6/10], Step [200/1875], Loss: 0.009861959405740103\n",
      "Epoch [6/10], Step [300/1875], Loss: 0.015155940225720406\n",
      "Epoch [6/10], Step [400/1875], Loss: 0.020457663906613986\n",
      "Epoch [6/10], Step [500/1875], Loss: 0.025584206816057366\n",
      "Epoch [6/10], Step [600/1875], Loss: 0.03076918576111396\n",
      "Epoch [6/10], Step [700/1875], Loss: 0.03649148706644773\n",
      "Epoch [6/10], Step [800/1875], Loss: 0.04195799835373958\n",
      "Epoch [6/10], Step [900/1875], Loss: 0.0463687755698959\n",
      "Epoch [6/10], Step [1000/1875], Loss: 0.05165018402437369\n",
      "Epoch [6/10], Step [1100/1875], Loss: 0.056928831419100365\n",
      "Epoch [6/10], Step [1200/1875], Loss: 0.06251095772857468\n"
     ]
    }
   ],
   "source": [
    "for i, net in enumerate(networks):\n",
    "    print(f\"Treinando Rede Neural {i + 1} com os seguintes parâmetros:\")\n",
    "    print(grid[i])\n",
    "\n",
    "    start = time.time()\n",
    "    train_model(net, criterion, optimizers[i], epochs)\n",
    "    end = time.time()\n",
    "    total_time = round(end - start, 1)\n",
    "\n",
    "    accuracy, loss, precision, recall, f1 = test_model(net, criterion)\n",
    "    print(f'Accuracy: {100 * accuracy}% Test Loss: {loss:.4f} Precision: {precision:.4f} Recall: {recall:.4f} F1 Score: {f1:.4f} Fit Time: {total_time}')\n",
    "    print('------------------------------------\\n')\n",
    "\n",
    "    class_accuracies = accuracy_classes(net)\n",
    "\n",
    "    model_data  = {\n",
    "        'network': [], #colocar como é a rede, podemos criar um nome pra cada rede e por ou algo do genero\n",
    "        **grid[i],\n",
    "        'fit_time': total_time,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'overall_accuracy ': accuracy,\n",
    "        'loss': loss,\n",
    "        'total_epochs': epochs,\n",
    "        'learning_rate': learning_rate,\n",
    "    }\n",
    "\n",
    "    for idx, acc in enumerate(class_accuracies):\n",
    "        model_data[f'accuracy_{idx}'] = acc\n",
    "\n",
    "    # Adicionando o modelo ao dataframe de modelos\n",
    "    model_df.loc[len(model_df)] = model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df.to_csv('cnn_models_metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando modelo treinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseCNN(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=6272, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# path_model_trained = ''\n",
    "\n",
    "model_trained = BaseCNN()\n",
    "model_trained.load_state_dict(torch.load(path_model_trained))\n",
    "model_trained.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prevendo a classe de uma entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classe prevista: 9\n"
     ]
    }
   ],
   "source": [
    "image_path = 'imagens_teste\\imagem_teste_4.png'\n",
    "\n",
    "predict_image(image_path, model_trained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analisando acurácia de cada classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for class: 0 is 99.18 %\n",
      "Accuracy for class: 1 is 99.12 %\n",
      "Accuracy for class: 2 is 97.29 %\n",
      "Accuracy for class: 3 is 98.12 %\n",
      "Accuracy for class: 4 is 98.37 %\n",
      "Accuracy for class: 5 is 97.87 %\n",
      "Accuracy for class: 6 is 97.70 %\n",
      "Accuracy for class: 7 is 97.37 %\n",
      "Accuracy for class: 8 is 97.23 %\n",
      "Accuracy for class: 9 is 96.53 %\n"
     ]
    }
   ],
   "source": [
    "all_accuracy = accuracy_classes(model_trained)\n",
    "for i, accuracy in enumerate(all_accuracy):\n",
    "    print(f'Accuracy for class: {i} is {accuracy:.2f} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
