{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os, datetime, torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms \n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando e seperando dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# Transformações para normalizar o dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Carregando o dataset MNIST\n",
    "trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# DataLoader para o dataset de treinamento e teste\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "classes = list(map(str, range(0, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualizando dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(dl):\n",
    "    for images, labels in dl:\n",
    "        fig, ax = plt.subplots(figsize=(12, 12))\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "        ax.imshow(make_grid(images[:64], nrow=8).permute(1, 2, 0))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x1200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_batch(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x1200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_batch(testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando uma instância da classe nn.Module para criar redes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explicando cada parâmetro da camada convolucional:\n",
    "\n",
    "- **in_channels**: Quantos canais de cores os inputs possuem. No caso de imagens em preto e branco, como estamos trabalhando com MNIST, há apenas um canal de cor.\n",
    "\n",
    "- **out_channels**: Número de filtros (kernels) que serão aplicados à imagem durante a convolução. Cada filtro é responsável por extrair características latentes da imagem.\n",
    "\n",
    "- **kernel_size**: Dimensão do filtro utilizado na convolução. O valor comum de 3 é amplamente usado, pois é suficiente para capturar detalhes locais, ao mesmo tempo em que percebe padrões maiores na imagem.\n",
    "\n",
    "- **stride**: Indica de quanto em quantos pixels o filtro será aplicado na imagem.\n",
    "\n",
    "- **padding**: Adiciona pixels em volta da imagem de entrada durante a convolução para garantir que o tamanho da saída após a convolução permaneça o mesmo que o da entrada.\n",
    "  - *Observação*: Ao adicionar padding, é preciso ter cuidado com os valores para garantir que a resolução e o tamanho da imagem não sejam afetados de forma indesejada.\n",
    "\n",
    "- **dropout_prob**: Probabilidade de que um neurônio seja desligado durante o treinamento da rede. Isso é uma técnica de regularização que ajuda a prevenir o overfitting, forçando a rede a aprender representações mais robustas e generalizáveis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fórmulas das Dimensões de Saída\n",
    "\n",
    "A fórmula para calcular a dimensão de saída \\( W_out \\) da convolução é:\n",
    "\n",
    "$$\n",
    "W_{\\text{out}} = \\left\\lfloor \\frac{W_{\\text{in}} + 2 \\times \\text{padding} - \\text{kernel\\_size}}{\\text{stride}} \\right\\rfloor + 1\n",
    "$$\n",
    "\n",
    "E a fórmula para calcular a dimensão de saída \\( H_out \\) da convolução é:\n",
    "\n",
    "$$\n",
    "H_{\\text{out}} = \\left\\lfloor \\frac{H_{\\text{in}} + 2 \\times \\text{padding} - \\text{kernel\\_size}}{\\text{stride}} \\right\\rfloor + 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resumindo as trasnformações:\n",
    "\n",
    "- Entrada Original: 28x28 pixels, 1 canal.\n",
    "- Após Convolução: 28x28 pixels, 32 canais.\n",
    "- Após Pooling: 14x14 pixels, 32 canais.\n",
    "- Entrada para self.fc1: 32 × 14 × 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseCNN(nn.Module):\n",
    "    def __init__(self, conv_kernel_size=3, conv_stride=1, conv_padding=1, pool_kernel_size=2, pool_stride=2, dropout_prob=0.5):\n",
    "        super(BaseCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=conv_kernel_size, stride=conv_stride, padding=conv_padding)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=pool_kernel_size, stride=pool_stride)\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        self.fc1 = nn.Linear(32 * 14 * 14, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(-1, 32 * 14 * 14)\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepCNN(nn.Module):\n",
    "    def __init__(self, conv_kernel_size=3, conv_stride=1, conv_padding=1, pool_kernel_size=2, pool_stride=2, dropout_prob=0.5):\n",
    "        super(DeepCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=conv_kernel_size, stride=conv_stride, padding=conv_padding)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=conv_kernel_size, stride=conv_stride, padding=conv_padding)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=pool_kernel_size, stride=pool_stride)\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, trial, num_conv_layers, num_filters, num_neurons, drop_conv2, drop_prob_fc1):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        in_size = 28\n",
    "        conv_kernel_size = 3\n",
    "        # conv_kernel_stride\n",
    "        pool_kernel_size = 2\n",
    "        # pool_stride\n",
    "\n",
    "        # TODO(pedro): Try to make both the conv and pool kernel sizes constructor parameters.\n",
    "\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, num_filters[0],\n",
    "                                              kernel_size=(conv_kernel_size, conv_kernel_size))])\n",
    "        out_size = int((in_size - conv_kernel_size + 1) / 2)\n",
    "\n",
    "        for i in range(1, num_conv_layers):\n",
    "            self.convs.append(nn.Conv2d(in_channels=num_filters[i - 1], out_channels=num_filters[i],\n",
    "                                        kernel_size=(conv_kernel_size, conv_kernel_size)))\n",
    "            out_size = int((out_size - conv_kernel_size + 1) / 2)\n",
    "\n",
    "        self.conv2_drop = nn.Dropout2d(p=drop_conv2)\n",
    "        self.out_feature = num_filters[num_conv_layers - 1] * out_size * out_size\n",
    "        self.pool = nn.MaxPool2d(kernel_size=pool_kernel_size)\n",
    "        self.fc1 = nn.Linear(self.out_feature, num_neurons)\n",
    "        self.fc2 = nn.Linear(num_neurons, 10)\n",
    "        self.fc1_drop_prob = drop_prob_fc1\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x)\n",
    "            if i == 2:\n",
    "                x = self.conv2_drop(x)\n",
    "            x = F.relu(self.pool(x))\n",
    "\n",
    "        x = x.view(-1, self.out_feature)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, p=self.fc1_drop_prob, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definindo funções úteis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Função de treinamento do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, epochs):\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(trainloader):\n",
    "            (inputs, labels) = (inputs.to(device), labels.to(device))\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()  # Propaga os gradientes\n",
    "            optimizer.step()  # Atualiza os pesos\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f'Epoch [{epoch + 1}/{epochs}], Step [{i + 1}/{len(trainloader)}], Loss: {running_loss / len(trainloader)}')\n",
    "    \n",
    "    print('Finished Training!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Função de teste do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, criterion):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    accuracy = correct / total\n",
    "    loss = test_loss / len(testloader)\n",
    "    precision = precision_score(all_labels, all_predictions, average='weighted')\n",
    "    recall = recall_score(all_labels, all_predictions, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "\n",
    "    return accuracy, loss, precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Função de acurácia de cada classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mudar\n",
    "def accuracy_classes(model_trained):\n",
    "    correct_pred = {classname: 0 for classname in classes}\n",
    "    total_pred = {classname: 0 for classname in classes}\n",
    "    all_accuracy = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model_trained(images)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            # collect the correct predictions for each class\n",
    "            for label, prediction in zip(labels, predictions):\n",
    "                if label == prediction:\n",
    "                    correct_pred[classes[label]] += 1\n",
    "                total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "    for classname, correct_count in correct_pred.items():\n",
    "        accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "        all_accuracy.append(accuracy)\n",
    "    \n",
    "    return all_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Função para prever classe de uma imagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(image_path, model):\n",
    "    image = Image.open(image_path).convert('L')  \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((28, 28)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    # Adiciona dimensão de batch (1 imagem)\n",
    "    image = transform(image).unsqueeze(0) \n",
    "\n",
    "    # Passando a imagem pela rede\n",
    "    output = model(image).to(device)\n",
    "\n",
    "    _, predicted_class = torch.max(output, 1)\n",
    "    print(\"Classe prevista:\", predicted_class.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definindo cvs com os parametros e metricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (os.path.exists('cnn_models_metrics.csv')):\n",
    "    model_df = pd.read_csv('cnn_models_metrics.csv')\n",
    "else:\n",
    "    model_data = {\n",
    "        'network': [],\n",
    "        'conv_kernel_size': [],\n",
    "        'conv_stride': [],\n",
    "        'conv_padding': [],\n",
    "        'pool_kernel_size': [],\n",
    "        'pool_stride': [],\n",
    "        'dropout_prob': [],\n",
    "        'fit_time': [],\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1_score': [],\n",
    "        'overall_accuracy ': [],\n",
    "        'accuracy_0': [],\n",
    "        'accuracy_1': [],\n",
    "        'accuracy_2': [],\n",
    "        'accuracy_3': [],\n",
    "        'accuracy_4': [],\n",
    "        'accuracy_5': [],\n",
    "        'accuracy_6': [],\n",
    "        'accuracy_7': [],\n",
    "        'accuracy_8': [],\n",
    "        'accuracy_9': [],\n",
    "        'total_epochs': [],\n",
    "        'learning_rate': [],\n",
    "    }\n",
    "    model_df = pd.DataFrame(model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-21 17:29:40,488] A new study created in memory with name: no-name-03c9356e-6af0-49ef-8c36-cc9e0f97afb9\n",
      "/tmp/ipykernel_4782/466808286.py:32: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  num_filters = [int(trial.suggest_discrete_uniform(\"num_filter_\" + str(i), 16, 128, 16))\n",
      "/tmp/ipykernel_4782/466808286.py:34: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  num_neurons = trial.suggest_int(\"num_neurons\", 10, 400, 10)  # Number of neurons of fully connected layer 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 1.6259, Test Loss: 1.0676\n",
      "Epoch 2, Train Loss: 0.9868, Test Loss: 0.6157\n",
      "Epoch 3, Train Loss: 0.7390, Test Loss: 0.4467\n",
      "Epoch 4, Train Loss: 0.6199, Test Loss: 0.3556\n",
      "Epoch 5, Train Loss: 0.5468, Test Loss: 0.3022\n",
      "Epoch 6, Train Loss: 0.4940, Test Loss: 0.2682\n",
      "Epoch 7, Train Loss: 0.4564, Test Loss: 0.2410\n",
      "Epoch 8, Train Loss: 0.4250, Test Loss: 0.2198\n",
      "Epoch 9, Train Loss: 0.3964, Test Loss: 0.2034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-21 17:32:09,342] Trial 0 finished with value: 0.9491 and parameters: {'num_conv_layers': 1, 'num_filter_0': 64.0, 'num_neurons': 20, 'drop_conv2': 0.23734938701216274, 'drop_prob_fc1': 0.2543174378176739, 'optimizer': 'Adam', 'lr': 1.0973416268349785e-05}. Best is trial 0 with value: 0.9491.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Train Loss: 0.3740, Test Loss: 0.1904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4782/466808286.py:32: FutureWarning: suggest_discrete_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., step=...) instead.\n",
      "  num_filters = [int(trial.suggest_discrete_uniform(\"num_filter_\" + str(i), 16, 128, 16))\n",
      "/tmp/ipykernel_4782/466808286.py:34: FutureWarning: suggest_int() got {'step'} as positional arguments but they were expected to be given as keyword arguments.\n",
      "  num_neurons = trial.suggest_int(\"num_neurons\", 10, 400, 10)  # Number of neurons of fully connected layer 1\n",
      "/home/guilhermemaciel/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 2.3008, Test Loss: 2.2988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guilhermemaciel/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss: 2.2986, Test Loss: 2.2967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guilhermemaciel/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Loss: 2.2965, Test Loss: 2.2946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guilhermemaciel/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train Loss: 2.2946, Test Loss: 2.2925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guilhermemaciel/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Train Loss: 2.2925, Test Loss: 2.2905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guilhermemaciel/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Train Loss: 2.2904, Test Loss: 2.2885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guilhermemaciel/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Train Loss: 2.2883, Test Loss: 2.2866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guilhermemaciel/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Train Loss: 2.2863, Test Loss: 2.2846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guilhermemaciel/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Train Loss: 2.2844, Test Loss: 2.2826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guilhermemaciel/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "[I 2024-05-21 17:34:46,948] Trial 1 finished with value: 0.1326 and parameters: {'num_conv_layers': 2, 'num_filter_0': 64.0, 'num_filter_1': 48.0, 'num_neurons': 120, 'drop_conv2': 0.4523208662683602, 'drop_prob_fc1': 0.271317494693467, 'optimizer': 'SGD', 'lr': 1.3769435811053966e-05}. Best is trial 0 with value: 0.9491.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Train Loss: 2.2825, Test Loss: 2.2806\n",
      "\n",
      "-- Study Statistics --\n",
      "  Number of finished trials: 2\n",
      "  Number of pruned trials:   0\n",
      "  Number of complete trials: 2\n",
      "\n",
      "-- Best Trial --\n",
      "  Accuracy:  0.9491\n",
      "  Test Loss: 0.19037811377773078\n",
      "  Precision: 0.9490754338838602\n",
      "  Recall:    0.9491\n",
      "  F1 Score:  0.9489642645424615\n",
      "  Parameters: \n",
      "    num_conv_layers: 1\n",
      "    num_filter_0:    64.0\n",
      "    num_neurons:     20\n",
      "    drop_conv2:      0.23734938701216274\n",
      "    drop_prob_fc1:   0.2543174378176739\n",
      "    optimizer:       Adam\n",
      "    lr:              1.0973416268349785e-05\n",
      "\n",
      "-- Overall Results (Ordered by Accuracy) --\n",
      "   number   value  params_drop_conv2  params_drop_prob_fc1  params_lr  \\\n",
      "1       1  0.1326           0.452321              0.271317   0.000014   \n",
      "0       0  0.9491           0.237349              0.254317   0.000011   \n",
      "\n",
      "   params_num_conv_layers  params_num_filter_0  params_num_filter_1  \\\n",
      "1                       2                 64.0                 48.0   \n",
      "0                       1                 64.0                  NaN   \n",
      "\n",
      "   params_num_neurons params_optimizer  user_attrs_f1_score  \\\n",
      "1                 120              SGD             0.076039   \n",
      "0                  20             Adam             0.948964   \n",
      "\n",
      "   user_attrs_precision  user_attrs_recall  user_attrs_test_loss  \n",
      "1              0.252707             0.1326              2.280598  \n",
      "0              0.949075             0.9491              0.190378  \n",
      "\n",
      "-- Most Important Hyperparameters --\n",
      "  lr:              29.76%\n",
      "  num_neurons:     25.00%\n",
      "  num_filter_0:    20.24%\n",
      "  optimizer:       10.71%\n",
      "  drop_conv2:      7.14%\n",
      "  drop_prob_fc1:   4.76%\n",
      "  num_conv_layers: 2.38%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "num_trials = 2\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# TODO(pedro): Try to generate different types of criterion inside the objective function.\n",
    "\n",
    "# TODO(pedro): Factor out this function eventually.\n",
    "def train_model_once(model, criterion, optimizer):\n",
    "    model.train().to(device)\n",
    "\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in trainloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()  # Propaga os gradientes\n",
    "        optimizer.step()  # Atualiza os pesos\n",
    "\n",
    "        running_loss += loss.item()\n",
    " \n",
    "    return running_loss / len(trainloader)\n",
    "\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "\n",
    "def objective(trial):\n",
    "    # Define range of values to be tested for the hyperparameters\n",
    "    num_conv_layers = trial.suggest_int(\"num_conv_layers\", 1, 3)\n",
    "    num_filters = [int(trial.suggest_discrete_uniform(\"num_filter_\" + str(i), 16, 128, 16))\n",
    "                   for i in range(num_conv_layers)]\n",
    "    num_neurons = trial.suggest_int(\"num_neurons\", 10, 400, 10)  # Number of neurons of fully connected layer 1\n",
    "    drop_conv2 = trial.suggest_float(\"drop_conv2\", 0.2, 0.5)     # Dropout for convolutional layer 2\n",
    "    drop_fc1 = trial.suggest_float(\"drop_prob_fc1\", 0.2, 0.5)    # Dropout probability for fully connected layer 1\n",
    "\n",
    "    # Generate the model\n",
    "    model = CNN(trial, num_conv_layers, num_filters, num_neurons, drop_conv2,  drop_fc1).to(device)\n",
    "\n",
    "    # Generate the optimizer\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
    "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_model_once(model, criterion, optimizer)\n",
    "        accuracy, test_loss, precision, recall, f1 = test_model(model, criterion)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, \" +\n",
    "              f\"Train Loss: {train_loss:.4f}, \" + \n",
    "              f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "        # Pruning (stops trial early if not promising)\n",
    "        trial.report(accuracy, epoch)\n",
    "        # Handle pruning based on the intermediate value.\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    trial.set_user_attr(\"test_loss\", test_loss)\n",
    "    trial.set_user_attr(\"precision\", precision)\n",
    "    trial.set_user_attr(\"recall\", recall)\n",
    "    trial.set_user_attr(\"f1_score\", f1)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=num_trials)\n",
    "\n",
    "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "print(\"\\n-- Study Statistics --\")\n",
    "print(f\"  Number of finished trials: {len(study.trials)}\")\n",
    "print(f\"  Number of pruned trials:   {len(pruned_trials)}\")\n",
    "print(f\"  Number of complete trials: {len(complete_trials)}\")\n",
    "\n",
    "best_trial = study.best_trial\n",
    "print(\"\\n-- Best Trial --\")\n",
    "print(f\"  Accuracy:  {best_trial.value}\")\n",
    "print(f\"  Test Loss: {best_trial.user_attrs['test_loss']}\")\n",
    "print(f\"  Precision: {best_trial.user_attrs['precision']}\")\n",
    "print(f\"  Recall:    {best_trial.user_attrs['recall']}\")\n",
    "print(f\"  F1 Score:  {best_trial.user_attrs['f1_score']}\")\n",
    "print(\"  Parameters: \")\n",
    "for key, val in best_trial.params.items():\n",
    "    print(f\"    {key}: {(15 - len(key)) * ' '}{val}\")\n",
    "\n",
    "# Save results to csv file\n",
    "df = study.trials_dataframe().drop([\"datetime_start\", \"datetime_complete\", \"duration\"], axis=1)\n",
    "df = df.loc[df[\"state\"] == \"COMPLETE\"]  # Keep only results that did not prune\n",
    "df = df.drop(\"state\", axis=1)\n",
    "df = df.sort_values(\"value\")            # Sort based on accuracy\n",
    "df.to_csv(\"optuna_results.csv\", index=False)\n",
    "\n",
    "print(f\"\\n-- Overall Results (Ordered by Accuracy) --\")\n",
    "print(df)\n",
    "\n",
    "most_important_parameters = optuna.importance.get_param_importances(study, target=None)\n",
    "\n",
    "print(\"\\n-- Most Important Hyperparameters --\")\n",
    "for key, val in most_important_parameters.items():\n",
    "    print(f\"  {key}: {(15 - len(key)) * ' '}{(100 * val):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instânciando o modelo base, definindo parametros, loss function e otimizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaseCNN().to(device)\n",
    "\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treinamento o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/1875], Loss: 0.06164853261311849\n",
      "Epoch [1/10], Step [200/1875], Loss: 0.08984224991003673\n",
      "Epoch [1/10], Step [300/1875], Loss: 0.11470057706832885\n",
      "Epoch [1/10], Step [400/1875], Loss: 0.13666963748931885\n",
      "Epoch [1/10], Step [500/1875], Loss: 0.15664172879854837\n",
      "Epoch [1/10], Step [600/1875], Loss: 0.17653292830785117\n",
      "Epoch [1/10], Step [700/1875], Loss: 0.19589433399836223\n",
      "Epoch [1/10], Step [800/1875], Loss: 0.21406442813475926\n",
      "Epoch [1/10], Step [900/1875], Loss: 0.23262717601458233\n",
      "Epoch [1/10], Step [1000/1875], Loss: 0.24996402680873872\n",
      "Epoch [1/10], Step [1100/1875], Loss: 0.2664201853553454\n",
      "Epoch [1/10], Step [1200/1875], Loss: 0.28292104640801746\n",
      "Epoch [1/10], Step [1300/1875], Loss: 0.29843647707303367\n",
      "Epoch [1/10], Step [1400/1875], Loss: 0.3146368290623029\n",
      "Epoch [1/10], Step [1500/1875], Loss: 0.3298328004995982\n",
      "Epoch [1/10], Step [1600/1875], Loss: 0.3432939299682776\n",
      "Epoch [1/10], Step [1700/1875], Loss: 0.35818382936120036\n",
      "Epoch [1/10], Step [1800/1875], Loss: 0.37165542352199554\n",
      "Epoch [2/10], Step [100/1875], Loss: 0.01202691818078359\n",
      "Epoch [2/10], Step [200/1875], Loss: 0.02538488322297732\n",
      "Epoch [2/10], Step [300/1875], Loss: 0.03817179133097331\n",
      "Epoch [2/10], Step [400/1875], Loss: 0.05169079140623411\n",
      "Epoch [2/10], Step [500/1875], Loss: 0.06416944391727447\n",
      "Epoch [2/10], Step [600/1875], Loss: 0.0757680413365364\n",
      "Epoch [2/10], Step [700/1875], Loss: 0.088457056025664\n",
      "Epoch [2/10], Step [800/1875], Loss: 0.10287293252150218\n",
      "Epoch [2/10], Step [900/1875], Loss: 0.11608447500268618\n",
      "Epoch [2/10], Step [1000/1875], Loss: 0.1300203367114067\n",
      "Epoch [2/10], Step [1100/1875], Loss: 0.14196008006532987\n",
      "Epoch [2/10], Step [1200/1875], Loss: 0.15444372196396192\n",
      "Epoch [2/10], Step [1300/1875], Loss: 0.16519783385594686\n",
      "Epoch [2/10], Step [1400/1875], Loss: 0.1779115701218446\n",
      "Epoch [2/10], Step [1500/1875], Loss: 0.18968365872104961\n",
      "Epoch [2/10], Step [1600/1875], Loss: 0.20137854693730672\n",
      "Epoch [2/10], Step [1700/1875], Loss: 0.2127910896341006\n",
      "Epoch [2/10], Step [1800/1875], Loss: 0.22407008594870567\n",
      "Epoch [3/10], Step [100/1875], Loss: 0.01131915275255839\n",
      "Epoch [3/10], Step [200/1875], Loss: 0.021584721450010936\n",
      "Epoch [3/10], Step [300/1875], Loss: 0.031118872567017872\n",
      "Epoch [3/10], Step [400/1875], Loss: 0.04159349003632863\n",
      "Epoch [3/10], Step [500/1875], Loss: 0.05196670783360799\n",
      "Epoch [3/10], Step [600/1875], Loss: 0.0634334865818421\n",
      "Epoch [3/10], Step [700/1875], Loss: 0.0734901309510072\n",
      "Epoch [3/10], Step [800/1875], Loss: 0.08363506732781728\n",
      "Epoch [3/10], Step [900/1875], Loss: 0.09420511744121711\n",
      "Epoch [3/10], Step [1000/1875], Loss: 0.10479138519863287\n",
      "Epoch [3/10], Step [1100/1875], Loss: 0.11505992927253246\n",
      "Epoch [3/10], Step [1200/1875], Loss: 0.12420759210288525\n",
      "Epoch [3/10], Step [1300/1875], Loss: 0.13433263762195904\n",
      "Epoch [3/10], Step [1400/1875], Loss: 0.14397067203223704\n",
      "Epoch [3/10], Step [1500/1875], Loss: 0.15273476306001346\n",
      "Epoch [3/10], Step [1600/1875], Loss: 0.1618729174276193\n",
      "Epoch [3/10], Step [1700/1875], Loss: 0.17079852697749934\n",
      "Epoch [3/10], Step [1800/1875], Loss: 0.1816302311619123\n",
      "Epoch [4/10], Step [100/1875], Loss: 0.008697202294071515\n",
      "Epoch [4/10], Step [200/1875], Loss: 0.018002371316154796\n",
      "Epoch [4/10], Step [300/1875], Loss: 0.0271597255975008\n",
      "Epoch [4/10], Step [400/1875], Loss: 0.035522231487433116\n",
      "Epoch [4/10], Step [500/1875], Loss: 0.045400914532939596\n",
      "Epoch [4/10], Step [600/1875], Loss: 0.0549403261522452\n",
      "Epoch [4/10], Step [700/1875], Loss: 0.06468357952137788\n",
      "Epoch [4/10], Step [800/1875], Loss: 0.0738136700352033\n",
      "Epoch [4/10], Step [900/1875], Loss: 0.0817116104900837\n",
      "Epoch [4/10], Step [1000/1875], Loss: 0.09097125886480013\n",
      "Epoch [4/10], Step [1100/1875], Loss: 0.10026899658640226\n",
      "Epoch [4/10], Step [1200/1875], Loss: 0.10903151057163875\n",
      "Epoch [4/10], Step [1300/1875], Loss: 0.11715604090044895\n",
      "Epoch [4/10], Step [1400/1875], Loss: 0.12587329418907564\n",
      "Epoch [4/10], Step [1500/1875], Loss: 0.13414550102502107\n",
      "Epoch [4/10], Step [1600/1875], Loss: 0.14214089316775402\n",
      "Epoch [4/10], Step [1700/1875], Loss: 0.15062944452017546\n",
      "Epoch [4/10], Step [1800/1875], Loss: 0.15950830094466606\n",
      "Epoch [5/10], Step [100/1875], Loss: 0.00826336942811807\n",
      "Epoch [5/10], Step [200/1875], Loss: 0.01572716387708982\n",
      "Epoch [5/10], Step [300/1875], Loss: 0.023261946272850036\n",
      "Epoch [5/10], Step [400/1875], Loss: 0.031186774307489395\n",
      "Epoch [5/10], Step [500/1875], Loss: 0.03914538500706355\n",
      "Epoch [5/10], Step [600/1875], Loss: 0.04706458836197853\n",
      "Epoch [5/10], Step [700/1875], Loss: 0.05427496698697408\n",
      "Epoch [5/10], Step [800/1875], Loss: 0.0630549049427112\n",
      "Epoch [5/10], Step [900/1875], Loss: 0.07080075412491957\n",
      "Epoch [5/10], Step [1000/1875], Loss: 0.07831671423912048\n",
      "Epoch [5/10], Step [1100/1875], Loss: 0.0857166527012984\n",
      "Epoch [5/10], Step [1200/1875], Loss: 0.0931058104276657\n",
      "Epoch [5/10], Step [1300/1875], Loss: 0.10042413527071475\n",
      "Epoch [5/10], Step [1400/1875], Loss: 0.10825148823360602\n",
      "Epoch [5/10], Step [1500/1875], Loss: 0.11630154205958049\n",
      "Epoch [5/10], Step [1600/1875], Loss: 0.125054397491614\n",
      "Epoch [5/10], Step [1700/1875], Loss: 0.13274344365894794\n",
      "Epoch [5/10], Step [1800/1875], Loss: 0.14222793343365192\n",
      "Epoch [6/10], Step [100/1875], Loss: 0.007029141262173653\n",
      "Epoch [6/10], Step [200/1875], Loss: 0.013902568843960761\n",
      "Epoch [6/10], Step [300/1875], Loss: 0.021466352917750675\n",
      "Epoch [6/10], Step [400/1875], Loss: 0.0285758388876915\n",
      "Epoch [6/10], Step [500/1875], Loss: 0.035593740747869015\n",
      "Epoch [6/10], Step [600/1875], Loss: 0.04310408364087343\n",
      "Epoch [6/10], Step [700/1875], Loss: 0.050411294831335544\n",
      "Epoch [6/10], Step [800/1875], Loss: 0.05786258865445852\n",
      "Epoch [6/10], Step [900/1875], Loss: 0.06463640604813893\n",
      "Epoch [6/10], Step [1000/1875], Loss: 0.07145369990269343\n",
      "Epoch [6/10], Step [1100/1875], Loss: 0.07950313906570276\n",
      "Epoch [6/10], Step [1200/1875], Loss: 0.08650713019520044\n",
      "Epoch [6/10], Step [1300/1875], Loss: 0.09336836769133806\n",
      "Epoch [6/10], Step [1400/1875], Loss: 0.1002895756949981\n",
      "Epoch [6/10], Step [1500/1875], Loss: 0.10777090437958638\n",
      "Epoch [6/10], Step [1600/1875], Loss: 0.11492023238688708\n",
      "Epoch [6/10], Step [1700/1875], Loss: 0.12215161392043035\n",
      "Epoch [6/10], Step [1800/1875], Loss: 0.13026121723006168\n",
      "Epoch [7/10], Step [100/1875], Loss: 0.00568366252531608\n",
      "Epoch [7/10], Step [200/1875], Loss: 0.012434524822235107\n",
      "Epoch [7/10], Step [300/1875], Loss: 0.020271239123741784\n",
      "Epoch [7/10], Step [400/1875], Loss: 0.027040965404113135\n",
      "Epoch [7/10], Step [500/1875], Loss: 0.033664234539866444\n",
      "Epoch [7/10], Step [600/1875], Loss: 0.040467512712876005\n",
      "Epoch [7/10], Step [700/1875], Loss: 0.047708473391334216\n",
      "Epoch [7/10], Step [800/1875], Loss: 0.054900341435770196\n",
      "Epoch [7/10], Step [900/1875], Loss: 0.06184578305532535\n",
      "Epoch [7/10], Step [1000/1875], Loss: 0.06881713675806919\n",
      "Epoch [7/10], Step [1100/1875], Loss: 0.07562943520098925\n",
      "Epoch [7/10], Step [1200/1875], Loss: 0.08247105964422226\n",
      "Epoch [7/10], Step [1300/1875], Loss: 0.08885448832561572\n",
      "Epoch [7/10], Step [1400/1875], Loss: 0.09525894947598378\n",
      "Epoch [7/10], Step [1500/1875], Loss: 0.10215684196129442\n",
      "Epoch [7/10], Step [1600/1875], Loss: 0.1089679972447455\n",
      "Epoch [7/10], Step [1700/1875], Loss: 0.11540072302048406\n",
      "Epoch [7/10], Step [1800/1875], Loss: 0.12208950154309471\n",
      "Epoch [8/10], Step [100/1875], Loss: 0.006011419422427813\n",
      "Epoch [8/10], Step [200/1875], Loss: 0.012141241403917471\n",
      "Epoch [8/10], Step [300/1875], Loss: 0.018886182426909606\n",
      "Epoch [8/10], Step [400/1875], Loss: 0.025584786068896453\n",
      "Epoch [8/10], Step [500/1875], Loss: 0.03194741672774156\n",
      "Epoch [8/10], Step [600/1875], Loss: 0.03865050996070107\n",
      "Epoch [8/10], Step [700/1875], Loss: 0.045134967118750015\n",
      "Epoch [8/10], Step [800/1875], Loss: 0.05138337077945471\n",
      "Epoch [8/10], Step [900/1875], Loss: 0.05780015662610531\n",
      "Epoch [8/10], Step [1000/1875], Loss: 0.06411314051002263\n",
      "Epoch [8/10], Step [1100/1875], Loss: 0.07061986857652665\n",
      "Epoch [8/10], Step [1200/1875], Loss: 0.0774498976657788\n",
      "Epoch [8/10], Step [1300/1875], Loss: 0.08458338680466015\n",
      "Epoch [8/10], Step [1400/1875], Loss: 0.09039537890652816\n",
      "Epoch [8/10], Step [1500/1875], Loss: 0.0964574916779995\n",
      "Epoch [8/10], Step [1600/1875], Loss: 0.10219963147292535\n",
      "Epoch [8/10], Step [1700/1875], Loss: 0.10914863071242968\n",
      "Epoch [8/10], Step [1800/1875], Loss: 0.11512668451269467\n",
      "Epoch [9/10], Step [100/1875], Loss: 0.00551673044214646\n",
      "Epoch [9/10], Step [200/1875], Loss: 0.012126986794173717\n",
      "Epoch [9/10], Step [300/1875], Loss: 0.017791666900118193\n",
      "Epoch [9/10], Step [400/1875], Loss: 0.02341164084027211\n",
      "Epoch [9/10], Step [500/1875], Loss: 0.02969951221222679\n",
      "Epoch [9/10], Step [600/1875], Loss: 0.03583431200260918\n",
      "Epoch [9/10], Step [700/1875], Loss: 0.0431342263850073\n",
      "Epoch [9/10], Step [800/1875], Loss: 0.04955459393387039\n",
      "Epoch [9/10], Step [900/1875], Loss: 0.055966813720017675\n",
      "Epoch [9/10], Step [1000/1875], Loss: 0.061184942109137774\n",
      "Epoch [9/10], Step [1100/1875], Loss: 0.06648358372574051\n",
      "Epoch [9/10], Step [1200/1875], Loss: 0.07261884589120746\n",
      "Epoch [9/10], Step [1300/1875], Loss: 0.07833478094364206\n",
      "Epoch [9/10], Step [1400/1875], Loss: 0.08408805938834946\n",
      "Epoch [9/10], Step [1500/1875], Loss: 0.09069353402679166\n",
      "Epoch [9/10], Step [1600/1875], Loss: 0.09661259493057926\n",
      "Epoch [9/10], Step [1700/1875], Loss: 0.10243649559915066\n",
      "Epoch [9/10], Step [1800/1875], Loss: 0.10794938218742609\n",
      "Epoch [10/10], Step [100/1875], Loss: 0.0052056582277019816\n",
      "Epoch [10/10], Step [200/1875], Loss: 0.010772464668005705\n",
      "Epoch [10/10], Step [300/1875], Loss: 0.01635241039022803\n",
      "Epoch [10/10], Step [400/1875], Loss: 0.02158592089985808\n",
      "Epoch [10/10], Step [500/1875], Loss: 0.02829952313527465\n",
      "Epoch [10/10], Step [600/1875], Loss: 0.03463861238236229\n",
      "Epoch [10/10], Step [700/1875], Loss: 0.04028310642217597\n",
      "Epoch [10/10], Step [800/1875], Loss: 0.04595649822677175\n",
      "Epoch [10/10], Step [900/1875], Loss: 0.052559421651810406\n",
      "Epoch [10/10], Step [1000/1875], Loss: 0.05874656839693586\n",
      "Epoch [10/10], Step [1100/1875], Loss: 0.06448604376539588\n",
      "Epoch [10/10], Step [1200/1875], Loss: 0.07054226461797952\n",
      "Epoch [10/10], Step [1300/1875], Loss: 0.07566136632412672\n",
      "Epoch [10/10], Step [1400/1875], Loss: 0.08118998395899932\n",
      "Epoch [10/10], Step [1500/1875], Loss: 0.08745909309138854\n",
      "Epoch [10/10], Step [1600/1875], Loss: 0.0930620288344721\n",
      "Epoch [10/10], Step [1700/1875], Loss: 0.09813238962044318\n",
      "Epoch [10/10], Step [1800/1875], Loss: 0.10511358506331842\n",
      "Finished Training!\n"
     ]
    }
   ],
   "source": [
    "train_model(model, criterion, optimizer, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testando o modleo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.47%, Test Loss: 0.0818, Precision: 0.9748, Recall: 0.9747, F1 Score: 0.9747\n"
     ]
    }
   ],
   "source": [
    "accuracy, loss, precision, recall, f1 = test_model(model, criterion)\n",
    "print(f'Accuracy: {100 * accuracy}%, Test Loss: {loss:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvando o modelo treinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('modelos_treinados'):\n",
    "    os.makedirs('modelos_treinados')\n",
    "\n",
    "current_datetime = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "path_model_trained = os.path.join('modelos_treinados', f'model_{current_datetime}.pth')\n",
    "\n",
    "torch.save(model.state_dict(), path_model_trained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Função de criação de derivação da rede base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn(conv_kernel_size, conv_stride, conv_padding, pool_kernel_size, pool_stride, dropout_prob):\n",
    "    class CNNTest(BaseCNN):\n",
    "        def __init__(self):\n",
    "            super(CNNTest, self).__init__(conv_kernel_size, conv_stride, conv_padding, pool_kernel_size, pool_stride, dropout_prob)\n",
    "    return CNNTest().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instanciando os modelos com grid de parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = [\n",
    "    {'conv_kernel_size': 3, 'conv_stride': 1, 'conv_padding': 1, 'pool_kernel_size': 2, 'pool_stride': 2, 'dropout_prob': 0.5},\n",
    "    {'conv_kernel_size': 5, 'conv_stride': 1, 'conv_padding': 2, 'pool_kernel_size': 2, 'pool_stride': 2, 'dropout_prob': 0.3},\n",
    "    {'conv_kernel_size': 3, 'conv_stride': 1, 'conv_padding': 1, 'pool_kernel_size': 2, 'pool_stride': 2, 'dropout_prob': 0.7}\n",
    "]\n",
    "networks = [create_cnn(**params) for params in grid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definindo parametros adicionais, loss function e otimizadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizers = [optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9) for model in networks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treinando e testando os modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinando Rede Neural 1 com os seguintes parâmetros:\n",
      "{'conv_kernel_size': 3, 'conv_stride': 1, 'conv_padding': 1, 'pool_kernel_size': 2, 'pool_stride': 2, 'dropout_prob': 0.5}\n",
      "Epoch [1/10], Step [100/1875], Loss: 0.005225570221741994\n",
      "Epoch [1/10], Step [200/1875], Loss: 0.010929282474021118\n",
      "Epoch [1/10], Step [300/1875], Loss: 0.01710455399006605\n",
      "Epoch [1/10], Step [400/1875], Loss: 0.022760026408235233\n",
      "Epoch [1/10], Step [500/1875], Loss: 0.028275868525356053\n",
      "Epoch [1/10], Step [600/1875], Loss: 0.034331384740024805\n",
      "Epoch [1/10], Step [700/1875], Loss: 0.0405867195216318\n",
      "Epoch [1/10], Step [800/1875], Loss: 0.046971213745822506\n",
      "Epoch [1/10], Step [900/1875], Loss: 0.052166199636707704\n",
      "Epoch [1/10], Step [1000/1875], Loss: 0.057462050500760475\n",
      "Epoch [1/10], Step [1100/1875], Loss: 0.06291218085375926\n",
      "Epoch [1/10], Step [1200/1875], Loss: 0.06783846076317132\n",
      "Epoch [1/10], Step [1300/1875], Loss: 0.0733626504290849\n",
      "Epoch [1/10], Step [1400/1875], Loss: 0.07935240673062702\n",
      "Epoch [1/10], Step [1500/1875], Loss: 0.08517194331772625\n",
      "Epoch [1/10], Step [1600/1875], Loss: 0.09076449838566283\n",
      "Epoch [1/10], Step [1700/1875], Loss: 0.09620107075658937\n",
      "Epoch [1/10], Step [1800/1875], Loss: 0.10267039297310014\n",
      "Epoch [2/10], Step [100/1875], Loss: 0.005885401843488216\n",
      "Epoch [2/10], Step [200/1875], Loss: 0.011172555364171664\n",
      "Epoch [2/10], Step [300/1875], Loss: 0.01654986491029461\n",
      "Epoch [2/10], Step [400/1875], Loss: 0.022146679121007522\n",
      "Epoch [2/10], Step [500/1875], Loss: 0.027691113091260194\n",
      "Epoch [2/10], Step [600/1875], Loss: 0.03321274068330725\n",
      "Epoch [2/10], Step [700/1875], Loss: 0.03929219618663192\n",
      "Epoch [2/10], Step [800/1875], Loss: 0.04545180574432015\n",
      "Epoch [2/10], Step [900/1875], Loss: 0.05044516787106792\n",
      "Epoch [2/10], Step [1000/1875], Loss: 0.05573381539608042\n",
      "Epoch [2/10], Step [1100/1875], Loss: 0.06121557537093759\n",
      "Epoch [2/10], Step [1200/1875], Loss: 0.0661945664110283\n",
      "Epoch [2/10], Step [1300/1875], Loss: 0.07199418007284403\n",
      "Epoch [2/10], Step [1400/1875], Loss: 0.07729519255633155\n",
      "Epoch [2/10], Step [1500/1875], Loss: 0.08349477084055543\n",
      "Epoch [2/10], Step [1600/1875], Loss: 0.08930464830274383\n",
      "Epoch [2/10], Step [1700/1875], Loss: 0.09490978518500924\n",
      "Epoch [2/10], Step [1800/1875], Loss: 0.10039098989292979\n",
      "Epoch [3/10], Step [100/1875], Loss: 0.00671915693283081\n",
      "Epoch [3/10], Step [200/1875], Loss: 0.011435210129370292\n",
      "Epoch [3/10], Step [300/1875], Loss: 0.016384442699700595\n",
      "Epoch [3/10], Step [400/1875], Loss: 0.021348325179268917\n",
      "Epoch [3/10], Step [500/1875], Loss: 0.02666514175410072\n",
      "Epoch [3/10], Step [600/1875], Loss: 0.03240044882272681\n",
      "Epoch [3/10], Step [700/1875], Loss: 0.03804072815477848\n",
      "Epoch [3/10], Step [800/1875], Loss: 0.04372126396199067\n",
      "Epoch [3/10], Step [900/1875], Loss: 0.048349026412765186\n",
      "Epoch [3/10], Step [1000/1875], Loss: 0.05422389035324256\n",
      "Epoch [3/10], Step [1100/1875], Loss: 0.059684952353934444\n",
      "Epoch [3/10], Step [1200/1875], Loss: 0.0656329721113046\n",
      "Epoch [3/10], Step [1300/1875], Loss: 0.07092273856873313\n",
      "Epoch [3/10], Step [1400/1875], Loss: 0.07627404345199466\n",
      "Epoch [3/10], Step [1500/1875], Loss: 0.08236507975608111\n",
      "Epoch [3/10], Step [1600/1875], Loss: 0.08723523094455402\n",
      "Epoch [3/10], Step [1700/1875], Loss: 0.09186851948946714\n",
      "Epoch [3/10], Step [1800/1875], Loss: 0.09772067161748807\n",
      "Epoch [4/10], Step [100/1875], Loss: 0.005112634098033111\n",
      "Epoch [4/10], Step [200/1875], Loss: 0.010181400196254253\n",
      "Epoch [4/10], Step [300/1875], Loss: 0.015738171697159607\n",
      "Epoch [4/10], Step [400/1875], Loss: 0.020664883281787238\n",
      "Epoch [4/10], Step [500/1875], Loss: 0.02642582556630174\n",
      "Epoch [4/10], Step [600/1875], Loss: 0.031399917077024775\n",
      "Epoch [4/10], Step [700/1875], Loss: 0.037446638346960145\n",
      "Epoch [4/10], Step [800/1875], Loss: 0.042927853702257074\n",
      "Epoch [4/10], Step [900/1875], Loss: 0.04786558529858788\n",
      "Epoch [4/10], Step [1000/1875], Loss: 0.05379577207217614\n",
      "Epoch [4/10], Step [1100/1875], Loss: 0.05851656615957618\n",
      "Epoch [4/10], Step [1200/1875], Loss: 0.06442032345409195\n",
      "Epoch [4/10], Step [1300/1875], Loss: 0.06972367670287688\n",
      "Epoch [4/10], Step [1400/1875], Loss: 0.07555250106751919\n",
      "Epoch [4/10], Step [1500/1875], Loss: 0.08053372088670731\n",
      "Epoch [4/10], Step [1600/1875], Loss: 0.08607957671235004\n",
      "Epoch [4/10], Step [1700/1875], Loss: 0.09168016001830498\n",
      "Epoch [4/10], Step [1800/1875], Loss: 0.0964365383900702\n",
      "Epoch [5/10], Step [100/1875], Loss: 0.004271226525306701\n",
      "Epoch [5/10], Step [200/1875], Loss: 0.009083940083285173\n",
      "Epoch [5/10], Step [300/1875], Loss: 0.013972856961687406\n",
      "Epoch [5/10], Step [400/1875], Loss: 0.019391659649461507\n",
      "Epoch [5/10], Step [500/1875], Loss: 0.02469506432811419\n",
      "Epoch [5/10], Step [600/1875], Loss: 0.029682804501553375\n",
      "Epoch [5/10], Step [700/1875], Loss: 0.03495618456602097\n",
      "Epoch [5/10], Step [800/1875], Loss: 0.040763356484100226\n",
      "Epoch [5/10], Step [900/1875], Loss: 0.046449263945842784\n",
      "Epoch [5/10], Step [1000/1875], Loss: 0.051736125527943176\n",
      "Epoch [5/10], Step [1100/1875], Loss: 0.05689936491958797\n",
      "Epoch [5/10], Step [1200/1875], Loss: 0.06209804269683858\n",
      "Epoch [5/10], Step [1300/1875], Loss: 0.06772927814337114\n",
      "Epoch [5/10], Step [1400/1875], Loss: 0.07288031076875826\n",
      "Epoch [5/10], Step [1500/1875], Loss: 0.07795096854207416\n",
      "Epoch [5/10], Step [1600/1875], Loss: 0.0830680890255918\n",
      "Epoch [5/10], Step [1700/1875], Loss: 0.08813621766443054\n",
      "Epoch [5/10], Step [1800/1875], Loss: 0.09311288846159975\n",
      "Epoch [6/10], Step [100/1875], Loss: 0.00538923430343469\n",
      "Epoch [6/10], Step [200/1875], Loss: 0.010168916792670886\n",
      "Epoch [6/10], Step [300/1875], Loss: 0.015090223246316115\n",
      "Epoch [6/10], Step [400/1875], Loss: 0.01980488656833768\n",
      "Epoch [6/10], Step [500/1875], Loss: 0.024305751301844914\n",
      "Epoch [6/10], Step [600/1875], Loss: 0.029793684520324073\n",
      "Epoch [6/10], Step [700/1875], Loss: 0.034059525414804614\n",
      "Epoch [6/10], Step [800/1875], Loss: 0.03931935200666388\n",
      "Epoch [6/10], Step [900/1875], Loss: 0.04462147899493575\n",
      "Epoch [6/10], Step [1000/1875], Loss: 0.05042726920271913\n",
      "Epoch [6/10], Step [1100/1875], Loss: 0.05623835921809077\n",
      "Epoch [6/10], Step [1200/1875], Loss: 0.06124034547097981\n",
      "Epoch [6/10], Step [1300/1875], Loss: 0.06600271664025884\n",
      "Epoch [6/10], Step [1400/1875], Loss: 0.07087544608674944\n",
      "Epoch [6/10], Step [1500/1875], Loss: 0.076038531351462\n",
      "Epoch [6/10], Step [1600/1875], Loss: 0.08097189533896744\n",
      "Epoch [6/10], Step [1700/1875], Loss: 0.08573368917765717\n",
      "Epoch [6/10], Step [1800/1875], Loss: 0.09073262669506173\n",
      "Epoch [7/10], Step [100/1875], Loss: 0.005069924962023894\n",
      "Epoch [7/10], Step [200/1875], Loss: 0.009800576074173053\n",
      "Epoch [7/10], Step [300/1875], Loss: 0.014198248717685541\n",
      "Epoch [7/10], Step [400/1875], Loss: 0.018946964604159195\n",
      "Epoch [7/10], Step [500/1875], Loss: 0.023985910582294066\n",
      "Epoch [7/10], Step [600/1875], Loss: 0.029012496494998534\n",
      "Epoch [7/10], Step [700/1875], Loss: 0.03482343114217122\n",
      "Epoch [7/10], Step [800/1875], Loss: 0.03975543421258529\n",
      "Epoch [7/10], Step [900/1875], Loss: 0.04394358957260847\n",
      "Epoch [7/10], Step [1000/1875], Loss: 0.04931315429310004\n",
      "Epoch [7/10], Step [1100/1875], Loss: 0.05519390599280596\n",
      "Epoch [7/10], Step [1200/1875], Loss: 0.06032670064419508\n",
      "Epoch [7/10], Step [1300/1875], Loss: 0.06507063343090316\n",
      "Epoch [7/10], Step [1400/1875], Loss: 0.06965561786803107\n",
      "Epoch [7/10], Step [1500/1875], Loss: 0.07406198879865308\n",
      "Epoch [7/10], Step [1600/1875], Loss: 0.07915685828166703\n",
      "Epoch [7/10], Step [1700/1875], Loss: 0.0845140811615934\n",
      "Epoch [7/10], Step [1800/1875], Loss: 0.09001195888159176\n",
      "Epoch [8/10], Step [100/1875], Loss: 0.004105823718756437\n",
      "Epoch [8/10], Step [200/1875], Loss: 0.008754593530048926\n",
      "Epoch [8/10], Step [300/1875], Loss: 0.013548322349786758\n",
      "Epoch [8/10], Step [400/1875], Loss: 0.018435650062312684\n",
      "Epoch [8/10], Step [500/1875], Loss: 0.022894873134543497\n",
      "Epoch [8/10], Step [600/1875], Loss: 0.027885495872795582\n",
      "Epoch [8/10], Step [700/1875], Loss: 0.032479719127714635\n",
      "Epoch [8/10], Step [800/1875], Loss: 0.0369715536557138\n",
      "Epoch [8/10], Step [900/1875], Loss: 0.04197655050953229\n",
      "Epoch [8/10], Step [1000/1875], Loss: 0.04714423503130674\n",
      "Epoch [8/10], Step [1100/1875], Loss: 0.05146086312184731\n",
      "Epoch [8/10], Step [1200/1875], Loss: 0.056569308467954396\n",
      "Epoch [8/10], Step [1300/1875], Loss: 0.06220493189816673\n",
      "Epoch [8/10], Step [1400/1875], Loss: 0.06703727927381793\n",
      "Epoch [8/10], Step [1500/1875], Loss: 0.0718460928009202\n",
      "Epoch [8/10], Step [1600/1875], Loss: 0.0771115238836656\n",
      "Epoch [8/10], Step [1700/1875], Loss: 0.08188211581744254\n",
      "Epoch [8/10], Step [1800/1875], Loss: 0.08668732446370025\n",
      "Epoch [9/10], Step [100/1875], Loss: 0.004899636672933897\n",
      "Epoch [9/10], Step [200/1875], Loss: 0.00923706443508466\n",
      "Epoch [9/10], Step [300/1875], Loss: 0.013986596129586299\n",
      "Epoch [9/10], Step [400/1875], Loss: 0.018497412642712395\n",
      "Epoch [9/10], Step [500/1875], Loss: 0.023185446480164924\n",
      "Epoch [9/10], Step [600/1875], Loss: 0.02794860572790106\n",
      "Epoch [9/10], Step [700/1875], Loss: 0.0323077769001325\n",
      "Epoch [9/10], Step [800/1875], Loss: 0.037343777752916016\n",
      "Epoch [9/10], Step [900/1875], Loss: 0.042229118510832386\n",
      "Epoch [9/10], Step [1000/1875], Loss: 0.04761659071023266\n",
      "Epoch [9/10], Step [1100/1875], Loss: 0.05230007058282693\n",
      "Epoch [9/10], Step [1200/1875], Loss: 0.056911337012797596\n",
      "Epoch [9/10], Step [1300/1875], Loss: 0.06129560483371218\n",
      "Epoch [9/10], Step [1400/1875], Loss: 0.06582487768270075\n",
      "Epoch [9/10], Step [1500/1875], Loss: 0.07066270048593482\n",
      "Epoch [9/10], Step [1600/1875], Loss: 0.07508207708944876\n",
      "Epoch [9/10], Step [1700/1875], Loss: 0.08059761922632655\n",
      "Epoch [9/10], Step [1800/1875], Loss: 0.08508354684909185\n",
      "Epoch [10/10], Step [100/1875], Loss: 0.00441206783503294\n",
      "Epoch [10/10], Step [200/1875], Loss: 0.009306562739113967\n",
      "Epoch [10/10], Step [300/1875], Loss: 0.013493883300075928\n",
      "Epoch [10/10], Step [400/1875], Loss: 0.017981372857342165\n",
      "Epoch [10/10], Step [500/1875], Loss: 0.02311341633722186\n",
      "Epoch [10/10], Step [600/1875], Loss: 0.028643020269150537\n",
      "Epoch [10/10], Step [700/1875], Loss: 0.033384277350331344\n",
      "Epoch [10/10], Step [800/1875], Loss: 0.037697574161613984\n",
      "Epoch [10/10], Step [900/1875], Loss: 0.04334671822252373\n",
      "Epoch [10/10], Step [1000/1875], Loss: 0.04736364512443542\n",
      "Epoch [10/10], Step [1100/1875], Loss: 0.051882745195925235\n",
      "Epoch [10/10], Step [1200/1875], Loss: 0.05608040701448917\n",
      "Epoch [10/10], Step [1300/1875], Loss: 0.06101520857512951\n",
      "Epoch [10/10], Step [1400/1875], Loss: 0.0659055128848801\n",
      "Epoch [10/10], Step [1500/1875], Loss: 0.0703340906072408\n",
      "Epoch [10/10], Step [1600/1875], Loss: 0.07534087254268428\n",
      "Epoch [10/10], Step [1700/1875], Loss: 0.07995876178182662\n",
      "Epoch [10/10], Step [1800/1875], Loss: 0.08495544591508805\n",
      "Finished Training!\n",
      "Accuracy: 97.92999999999999% Test Loss: 0.0654 Precision: 0.9793 Recall: 0.9793 F1 Score: 0.9793 Fit Time: 110.1\n",
      "------------------------------------\n",
      "\n",
      "Treinando Rede Neural 2 com os seguintes parâmetros:\n",
      "{'conv_kernel_size': 5, 'conv_stride': 1, 'conv_padding': 2, 'pool_kernel_size': 2, 'pool_stride': 2, 'dropout_prob': 0.3}\n",
      "Epoch [1/10], Step [100/1875], Loss: 0.05171921369234721\n",
      "Epoch [1/10], Step [200/1875], Loss: 0.07591705365180969\n",
      "Epoch [1/10], Step [300/1875], Loss: 0.09685356803337733\n",
      "Epoch [1/10], Step [400/1875], Loss: 0.11625352745850881\n",
      "Epoch [1/10], Step [500/1875], Loss: 0.1347769899169604\n",
      "Epoch [1/10], Step [600/1875], Loss: 0.15374163115819295\n",
      "Epoch [1/10], Step [700/1875], Loss: 0.17025052664279938\n",
      "Epoch [1/10], Step [800/1875], Loss: 0.18730278049707413\n",
      "Epoch [1/10], Step [900/1875], Loss: 0.20222162292400997\n",
      "Epoch [1/10], Step [1000/1875], Loss: 0.21783797700405122\n",
      "Epoch [1/10], Step [1100/1875], Loss: 0.23125957117875418\n",
      "Epoch [1/10], Step [1200/1875], Loss: 0.24668572638233502\n",
      "Epoch [1/10], Step [1300/1875], Loss: 0.26125886249343555\n",
      "Epoch [1/10], Step [1400/1875], Loss: 0.27557934267719586\n",
      "Epoch [1/10], Step [1500/1875], Loss: 0.288077550137043\n",
      "Epoch [1/10], Step [1600/1875], Loss: 0.30121761618852616\n",
      "Epoch [1/10], Step [1700/1875], Loss: 0.31330876693725584\n",
      "Epoch [1/10], Step [1800/1875], Loss: 0.32637105607191724\n",
      "Epoch [2/10], Step [100/1875], Loss: 0.010918285101652145\n",
      "Epoch [2/10], Step [200/1875], Loss: 0.022444103489319484\n",
      "Epoch [2/10], Step [300/1875], Loss: 0.032196738906701404\n",
      "Epoch [2/10], Step [400/1875], Loss: 0.04311279359658559\n",
      "Epoch [2/10], Step [500/1875], Loss: 0.054387782016396526\n",
      "Epoch [2/10], Step [600/1875], Loss: 0.0654271205097437\n",
      "Epoch [2/10], Step [700/1875], Loss: 0.07545303047597408\n",
      "Epoch [2/10], Step [800/1875], Loss: 0.08482329713801542\n",
      "Epoch [2/10], Step [900/1875], Loss: 0.09460666219592094\n",
      "Epoch [2/10], Step [1000/1875], Loss: 0.10452688859800498\n",
      "Epoch [2/10], Step [1100/1875], Loss: 0.11404999303619066\n",
      "Epoch [2/10], Step [1200/1875], Loss: 0.12341613160173098\n",
      "Epoch [2/10], Step [1300/1875], Loss: 0.13293036401867867\n",
      "Epoch [2/10], Step [1400/1875], Loss: 0.14214382026791572\n",
      "Epoch [2/10], Step [1500/1875], Loss: 0.15134246046145758\n",
      "Epoch [2/10], Step [1600/1875], Loss: 0.159591182744503\n",
      "Epoch [2/10], Step [1700/1875], Loss: 0.16901772795915604\n",
      "Epoch [2/10], Step [1800/1875], Loss: 0.17674875250359376\n",
      "Epoch [3/10], Step [100/1875], Loss: 0.007167588750521342\n",
      "Epoch [3/10], Step [200/1875], Loss: 0.01533819114267826\n",
      "Epoch [3/10], Step [300/1875], Loss: 0.02313944930136204\n",
      "Epoch [3/10], Step [400/1875], Loss: 0.03177365900675456\n",
      "Epoch [3/10], Step [500/1875], Loss: 0.040150779540340104\n",
      "Epoch [3/10], Step [600/1875], Loss: 0.048464836380382376\n",
      "Epoch [3/10], Step [700/1875], Loss: 0.05668392226447662\n",
      "Epoch [3/10], Step [800/1875], Loss: 0.06377634744495153\n",
      "Epoch [3/10], Step [900/1875], Loss: 0.07214996549934148\n",
      "Epoch [3/10], Step [1000/1875], Loss: 0.07941044266968965\n",
      "Epoch [3/10], Step [1100/1875], Loss: 0.08564502941469351\n",
      "Epoch [3/10], Step [1200/1875], Loss: 0.09283250194191933\n",
      "Epoch [3/10], Step [1300/1875], Loss: 0.10032144142140945\n",
      "Epoch [3/10], Step [1400/1875], Loss: 0.10804117158403\n",
      "Epoch [3/10], Step [1500/1875], Loss: 0.11465276788870493\n",
      "Epoch [3/10], Step [1600/1875], Loss: 0.12160708355456591\n",
      "Epoch [3/10], Step [1700/1875], Loss: 0.1288697893689076\n",
      "Epoch [3/10], Step [1800/1875], Loss: 0.13529583761096\n",
      "Epoch [4/10], Step [100/1875], Loss: 0.006678095092376073\n",
      "Epoch [4/10], Step [200/1875], Loss: 0.013281187149385611\n",
      "Epoch [4/10], Step [300/1875], Loss: 0.019868992357949415\n",
      "Epoch [4/10], Step [400/1875], Loss: 0.026279144581158955\n",
      "Epoch [4/10], Step [500/1875], Loss: 0.032623861254254975\n",
      "Epoch [4/10], Step [600/1875], Loss: 0.03894947290817897\n",
      "Epoch [4/10], Step [700/1875], Loss: 0.044826143566767375\n",
      "Epoch [4/10], Step [800/1875], Loss: 0.05159647069424391\n",
      "Epoch [4/10], Step [900/1875], Loss: 0.05793652616888285\n",
      "Epoch [4/10], Step [1000/1875], Loss: 0.06382605112145344\n",
      "Epoch [4/10], Step [1100/1875], Loss: 0.07007500566442808\n",
      "Epoch [4/10], Step [1200/1875], Loss: 0.07684377611577511\n",
      "Epoch [4/10], Step [1300/1875], Loss: 0.0833093228628238\n",
      "Epoch [4/10], Step [1400/1875], Loss: 0.0885466347883145\n",
      "Epoch [4/10], Step [1500/1875], Loss: 0.09436759200344483\n",
      "Epoch [4/10], Step [1600/1875], Loss: 0.0996905355806152\n",
      "Epoch [4/10], Step [1700/1875], Loss: 0.10582739653637012\n",
      "Epoch [4/10], Step [1800/1875], Loss: 0.11172607422918081\n",
      "Epoch [5/10], Step [100/1875], Loss: 0.005264685746779045\n",
      "Epoch [5/10], Step [200/1875], Loss: 0.010876302178700765\n",
      "Epoch [5/10], Step [300/1875], Loss: 0.016758760602772236\n",
      "Epoch [5/10], Step [400/1875], Loss: 0.02206366816436251\n",
      "Epoch [5/10], Step [500/1875], Loss: 0.02728674646218618\n",
      "Epoch [5/10], Step [600/1875], Loss: 0.03312173634171486\n",
      "Epoch [5/10], Step [700/1875], Loss: 0.038343878337740896\n",
      "Epoch [5/10], Step [800/1875], Loss: 0.04374740567356348\n",
      "Epoch [5/10], Step [900/1875], Loss: 0.04992477731158336\n",
      "Epoch [5/10], Step [1000/1875], Loss: 0.05530528291563193\n",
      "Epoch [5/10], Step [1100/1875], Loss: 0.06144724976370732\n",
      "Epoch [5/10], Step [1200/1875], Loss: 0.06678778236508369\n",
      "Epoch [5/10], Step [1300/1875], Loss: 0.07254627701466282\n",
      "Epoch [5/10], Step [1400/1875], Loss: 0.07842820654014747\n",
      "Epoch [5/10], Step [1500/1875], Loss: 0.08367660160760085\n",
      "Epoch [5/10], Step [1600/1875], Loss: 0.0893465385268132\n",
      "Epoch [5/10], Step [1700/1875], Loss: 0.09501888070503871\n",
      "Epoch [5/10], Step [1800/1875], Loss: 0.10037392097214858\n",
      "Epoch [6/10], Step [100/1875], Loss: 0.004996851872901122\n",
      "Epoch [6/10], Step [200/1875], Loss: 0.010384585785865784\n",
      "Epoch [6/10], Step [300/1875], Loss: 0.015717651416609683\n",
      "Epoch [6/10], Step [400/1875], Loss: 0.020681663623452185\n",
      "Epoch [6/10], Step [500/1875], Loss: 0.02529934140741825\n",
      "Epoch [6/10], Step [600/1875], Loss: 0.03063455638984839\n",
      "Epoch [6/10], Step [700/1875], Loss: 0.03498179435133934\n",
      "Epoch [6/10], Step [800/1875], Loss: 0.04004334016367793\n",
      "Epoch [6/10], Step [900/1875], Loss: 0.04445223416710893\n",
      "Epoch [6/10], Step [1000/1875], Loss: 0.04968667344525456\n",
      "Epoch [6/10], Step [1100/1875], Loss: 0.05464952239791552\n",
      "Epoch [6/10], Step [1200/1875], Loss: 0.05971231737261017\n",
      "Epoch [6/10], Step [1300/1875], Loss: 0.06524583324591318\n",
      "Epoch [6/10], Step [1400/1875], Loss: 0.07060733998864889\n",
      "Epoch [6/10], Step [1500/1875], Loss: 0.07635576580862204\n",
      "Epoch [6/10], Step [1600/1875], Loss: 0.08078045913303891\n",
      "Epoch [6/10], Step [1700/1875], Loss: 0.08593935183957219\n",
      "Epoch [6/10], Step [1800/1875], Loss: 0.09043586638271808\n",
      "Epoch [7/10], Step [100/1875], Loss: 0.004355842557176947\n",
      "Epoch [7/10], Step [200/1875], Loss: 0.008938130871330698\n",
      "Epoch [7/10], Step [300/1875], Loss: 0.013913684264943004\n",
      "Epoch [7/10], Step [400/1875], Loss: 0.018767703743403155\n",
      "Epoch [7/10], Step [500/1875], Loss: 0.02346046414785087\n",
      "Epoch [7/10], Step [600/1875], Loss: 0.02783697048040728\n",
      "Epoch [7/10], Step [700/1875], Loss: 0.032617068134869136\n",
      "Epoch [7/10], Step [800/1875], Loss: 0.03674219927725693\n",
      "Epoch [7/10], Step [900/1875], Loss: 0.0407036890356491\n",
      "Epoch [7/10], Step [1000/1875], Loss: 0.04559647076266507\n",
      "Epoch [7/10], Step [1100/1875], Loss: 0.05083100237436593\n",
      "Epoch [7/10], Step [1200/1875], Loss: 0.05518372923918068\n",
      "Epoch [7/10], Step [1300/1875], Loss: 0.05967625183872879\n",
      "Epoch [7/10], Step [1400/1875], Loss: 0.06536615292889376\n",
      "Epoch [7/10], Step [1500/1875], Loss: 0.07068693062923849\n",
      "Epoch [7/10], Step [1600/1875], Loss: 0.07549174743629992\n",
      "Epoch [7/10], Step [1700/1875], Loss: 0.07945123957283795\n",
      "Epoch [7/10], Step [1800/1875], Loss: 0.08349717468035718\n",
      "Epoch [8/10], Step [100/1875], Loss: 0.004240536266565323\n",
      "Epoch [8/10], Step [200/1875], Loss: 0.008487999514242013\n",
      "Epoch [8/10], Step [300/1875], Loss: 0.012802722202489774\n",
      "Epoch [8/10], Step [400/1875], Loss: 0.01709869836717844\n",
      "Epoch [8/10], Step [500/1875], Loss: 0.021890273542205494\n",
      "Epoch [8/10], Step [600/1875], Loss: 0.026284559425463278\n",
      "Epoch [8/10], Step [700/1875], Loss: 0.03188080999031663\n",
      "Epoch [8/10], Step [800/1875], Loss: 0.0366263385164241\n",
      "Epoch [8/10], Step [900/1875], Loss: 0.04035653216913342\n",
      "Epoch [8/10], Step [1000/1875], Loss: 0.04445273923700054\n",
      "Epoch [8/10], Step [1100/1875], Loss: 0.04940448221489787\n",
      "Epoch [8/10], Step [1200/1875], Loss: 0.05336725584367911\n",
      "Epoch [8/10], Step [1300/1875], Loss: 0.057998468272636335\n",
      "Epoch [8/10], Step [1400/1875], Loss: 0.06237818518045048\n",
      "Epoch [8/10], Step [1500/1875], Loss: 0.06641594366393984\n",
      "Epoch [8/10], Step [1600/1875], Loss: 0.07139447099678219\n",
      "Epoch [8/10], Step [1700/1875], Loss: 0.0758184417149673\n",
      "Epoch [8/10], Step [1800/1875], Loss: 0.07950005722902716\n",
      "Epoch [9/10], Step [100/1875], Loss: 0.003199749054014683\n",
      "Epoch [9/10], Step [200/1875], Loss: 0.007104385753969352\n",
      "Epoch [9/10], Step [300/1875], Loss: 0.011494733964775999\n",
      "Epoch [9/10], Step [400/1875], Loss: 0.015854600485786796\n",
      "Epoch [9/10], Step [500/1875], Loss: 0.01971890639116367\n",
      "Epoch [9/10], Step [600/1875], Loss: 0.023830476074044902\n",
      "Epoch [9/10], Step [700/1875], Loss: 0.02795695145788292\n",
      "Epoch [9/10], Step [800/1875], Loss: 0.03205541074685753\n",
      "Epoch [9/10], Step [900/1875], Loss: 0.03691544653934737\n",
      "Epoch [9/10], Step [1000/1875], Loss: 0.041164866649235285\n",
      "Epoch [9/10], Step [1100/1875], Loss: 0.04544104724129041\n",
      "Epoch [9/10], Step [1200/1875], Loss: 0.04978994850764672\n",
      "Epoch [9/10], Step [1300/1875], Loss: 0.053848218898226816\n",
      "Epoch [9/10], Step [1400/1875], Loss: 0.058047357231130206\n",
      "Epoch [9/10], Step [1500/1875], Loss: 0.06199533144682646\n",
      "Epoch [9/10], Step [1600/1875], Loss: 0.06601554387907187\n",
      "Epoch [9/10], Step [1700/1875], Loss: 0.07017229396576682\n",
      "Epoch [9/10], Step [1800/1875], Loss: 0.07410485143264135\n",
      "Epoch [10/10], Step [100/1875], Loss: 0.0034439975639184318\n",
      "Epoch [10/10], Step [200/1875], Loss: 0.007393537572274606\n",
      "Epoch [10/10], Step [300/1875], Loss: 0.011218844492609303\n",
      "Epoch [10/10], Step [400/1875], Loss: 0.01497282737828791\n",
      "Epoch [10/10], Step [500/1875], Loss: 0.01907566435150802\n",
      "Epoch [10/10], Step [600/1875], Loss: 0.023111451632653674\n",
      "Epoch [10/10], Step [700/1875], Loss: 0.027201005113124847\n",
      "Epoch [10/10], Step [800/1875], Loss: 0.03134540690407157\n",
      "Epoch [10/10], Step [900/1875], Loss: 0.0359112934773167\n",
      "Epoch [10/10], Step [1000/1875], Loss: 0.03981102422947685\n",
      "Epoch [10/10], Step [1100/1875], Loss: 0.04309273029739658\n",
      "Epoch [10/10], Step [1200/1875], Loss: 0.0474564468935132\n",
      "Epoch [10/10], Step [1300/1875], Loss: 0.05155234893746674\n",
      "Epoch [10/10], Step [1400/1875], Loss: 0.05530583169000844\n",
      "Epoch [10/10], Step [1500/1875], Loss: 0.05974128065655629\n",
      "Epoch [10/10], Step [1600/1875], Loss: 0.06377281008064747\n",
      "Epoch [10/10], Step [1700/1875], Loss: 0.06834605213863154\n",
      "Epoch [10/10], Step [1800/1875], Loss: 0.07179604267391065\n",
      "Finished Training!\n",
      "Accuracy: 98.0% Test Loss: 0.0628 Precision: 0.9801 Recall: 0.9800 F1 Score: 0.9800 Fit Time: 123.2\n",
      "------------------------------------\n",
      "\n",
      "Treinando Rede Neural 3 com os seguintes parâmetros:\n",
      "{'conv_kernel_size': 3, 'conv_stride': 1, 'conv_padding': 1, 'pool_kernel_size': 2, 'pool_stride': 2, 'dropout_prob': 0.7}\n",
      "Epoch [1/10], Step [100/1875], Loss: 0.0640008059978485\n",
      "Epoch [1/10], Step [200/1875], Loss: 0.09708963295618693\n",
      "Epoch [1/10], Step [300/1875], Loss: 0.12364679195086162\n",
      "Epoch [1/10], Step [400/1875], Loss: 0.15079785166978837\n",
      "Epoch [1/10], Step [500/1875], Loss: 0.17532648013035457\n",
      "Epoch [1/10], Step [600/1875], Loss: 0.1984533097823461\n",
      "Epoch [1/10], Step [700/1875], Loss: 0.22045711416403452\n",
      "Epoch [1/10], Step [800/1875], Loss: 0.24217379639148712\n",
      "Epoch [1/10], Step [900/1875], Loss: 0.26203356453577675\n",
      "Epoch [1/10], Step [1000/1875], Loss: 0.28185255197286607\n",
      "Epoch [1/10], Step [1100/1875], Loss: 0.3032194438815117\n",
      "Epoch [1/10], Step [1200/1875], Loss: 0.32459988173246385\n",
      "Epoch [1/10], Step [1300/1875], Loss: 0.34341348969141644\n",
      "Epoch [1/10], Step [1400/1875], Loss: 0.362611498606205\n",
      "Epoch [1/10], Step [1500/1875], Loss: 0.37913538377682365\n",
      "Epoch [1/10], Step [1600/1875], Loss: 0.39769725699822106\n",
      "Epoch [1/10], Step [1700/1875], Loss: 0.41566473315556846\n",
      "Epoch [1/10], Step [1800/1875], Loss: 0.4335957538565\n",
      "Epoch [2/10], Step [100/1875], Loss: 0.01688486952384313\n",
      "Epoch [2/10], Step [200/1875], Loss: 0.03326520376006762\n",
      "Epoch [2/10], Step [300/1875], Loss: 0.0501329948147138\n",
      "Epoch [2/10], Step [400/1875], Loss: 0.0655068979303042\n",
      "Epoch [2/10], Step [500/1875], Loss: 0.07910133619705836\n",
      "Epoch [2/10], Step [600/1875], Loss: 0.09378331207235654\n",
      "Epoch [2/10], Step [700/1875], Loss: 0.111116707243522\n",
      "Epoch [2/10], Step [800/1875], Loss: 0.12673551045457523\n",
      "Epoch [2/10], Step [900/1875], Loss: 0.1420855687558651\n",
      "Epoch [2/10], Step [1000/1875], Loss: 0.15704493874907494\n",
      "Epoch [2/10], Step [1100/1875], Loss: 0.17192754721442857\n",
      "Epoch [2/10], Step [1200/1875], Loss: 0.18817261119882267\n",
      "Epoch [2/10], Step [1300/1875], Loss: 0.20269997200369835\n",
      "Epoch [2/10], Step [1400/1875], Loss: 0.21735754661361376\n",
      "Epoch [2/10], Step [1500/1875], Loss: 0.2322306385099888\n",
      "Epoch [2/10], Step [1600/1875], Loss: 0.24611359457572302\n",
      "Epoch [2/10], Step [1700/1875], Loss: 0.26180172621806463\n",
      "Epoch [2/10], Step [1800/1875], Loss: 0.27450108871062595\n",
      "Epoch [3/10], Step [100/1875], Loss: 0.013654139110445976\n",
      "Epoch [3/10], Step [200/1875], Loss: 0.027594593704740208\n",
      "Epoch [3/10], Step [300/1875], Loss: 0.04015240065356095\n",
      "Epoch [3/10], Step [400/1875], Loss: 0.053040725152691204\n",
      "Epoch [3/10], Step [500/1875], Loss: 0.06709062125186126\n",
      "Epoch [3/10], Step [600/1875], Loss: 0.08018121108909448\n",
      "Epoch [3/10], Step [700/1875], Loss: 0.09381616623699665\n",
      "Epoch [3/10], Step [800/1875], Loss: 0.10593322007159392\n",
      "Epoch [3/10], Step [900/1875], Loss: 0.11898389526704947\n",
      "Epoch [3/10], Step [1000/1875], Loss: 0.1304967369904121\n",
      "Epoch [3/10], Step [1100/1875], Loss: 0.14303847186068694\n",
      "Epoch [3/10], Step [1200/1875], Loss: 0.15523845412035783\n",
      "Epoch [3/10], Step [1300/1875], Loss: 0.1675610528975725\n",
      "Epoch [3/10], Step [1400/1875], Loss: 0.17877658420105774\n",
      "Epoch [3/10], Step [1500/1875], Loss: 0.19256186847786108\n",
      "Epoch [3/10], Step [1600/1875], Loss: 0.20408076540331047\n",
      "Epoch [3/10], Step [1700/1875], Loss: 0.21717009987731775\n",
      "Epoch [3/10], Step [1800/1875], Loss: 0.22867844978074234\n",
      "Epoch [4/10], Step [100/1875], Loss: 0.011732837035258611\n",
      "Epoch [4/10], Step [200/1875], Loss: 0.02357224437991778\n",
      "Epoch [4/10], Step [300/1875], Loss: 0.03510576866666476\n",
      "Epoch [4/10], Step [400/1875], Loss: 0.04753495854288339\n",
      "Epoch [4/10], Step [500/1875], Loss: 0.058552007166047894\n",
      "Epoch [4/10], Step [600/1875], Loss: 0.06934332911521196\n",
      "Epoch [4/10], Step [700/1875], Loss: 0.0810457667345802\n",
      "Epoch [4/10], Step [800/1875], Loss: 0.09255564234008391\n",
      "Epoch [4/10], Step [900/1875], Loss: 0.10443537743439277\n",
      "Epoch [4/10], Step [1000/1875], Loss: 0.11539782992551724\n",
      "Epoch [4/10], Step [1100/1875], Loss: 0.1267739249840379\n",
      "Epoch [4/10], Step [1200/1875], Loss: 0.13792732284317413\n",
      "Epoch [4/10], Step [1300/1875], Loss: 0.14919733362446227\n",
      "Epoch [4/10], Step [1400/1875], Loss: 0.16120491731017828\n",
      "Epoch [4/10], Step [1500/1875], Loss: 0.1712144537255168\n",
      "Epoch [4/10], Step [1600/1875], Loss: 0.18270734979361294\n",
      "Epoch [4/10], Step [1700/1875], Loss: 0.19414056320637466\n",
      "Epoch [4/10], Step [1800/1875], Loss: 0.20472902215967576\n",
      "Epoch [5/10], Step [100/1875], Loss: 0.0098538069943587\n",
      "Epoch [5/10], Step [200/1875], Loss: 0.020301776499549548\n",
      "Epoch [5/10], Step [300/1875], Loss: 0.031213781531651816\n",
      "Epoch [5/10], Step [400/1875], Loss: 0.04168535656929016\n",
      "Epoch [5/10], Step [500/1875], Loss: 0.05203017718394597\n",
      "Epoch [5/10], Step [600/1875], Loss: 0.06292611084183057\n",
      "Epoch [5/10], Step [700/1875], Loss: 0.07339361261427403\n",
      "Epoch [5/10], Step [800/1875], Loss: 0.0838376240024964\n",
      "Epoch [5/10], Step [900/1875], Loss: 0.0936147186557452\n",
      "Epoch [5/10], Step [1000/1875], Loss: 0.10484221500356992\n",
      "Epoch [5/10], Step [1100/1875], Loss: 0.11582269273201624\n",
      "Epoch [5/10], Step [1200/1875], Loss: 0.12490885120133559\n",
      "Epoch [5/10], Step [1300/1875], Loss: 0.13561490862270195\n",
      "Epoch [5/10], Step [1400/1875], Loss: 0.14427132675846419\n",
      "Epoch [5/10], Step [1500/1875], Loss: 0.1543770964662234\n",
      "Epoch [5/10], Step [1600/1875], Loss: 0.1647361086865266\n",
      "Epoch [5/10], Step [1700/1875], Loss: 0.17396407386461893\n",
      "Epoch [5/10], Step [1800/1875], Loss: 0.18510729535122714\n",
      "Epoch [6/10], Step [100/1875], Loss: 0.010595733630657196\n",
      "Epoch [6/10], Step [200/1875], Loss: 0.020777804775039356\n",
      "Epoch [6/10], Step [300/1875], Loss: 0.030370091292262077\n",
      "Epoch [6/10], Step [400/1875], Loss: 0.0398937822530667\n",
      "Epoch [6/10], Step [500/1875], Loss: 0.050055489944418274\n",
      "Epoch [6/10], Step [600/1875], Loss: 0.059602976674834884\n",
      "Epoch [6/10], Step [700/1875], Loss: 0.07027915801803271\n",
      "Epoch [6/10], Step [800/1875], Loss: 0.07911436240971088\n",
      "Epoch [6/10], Step [900/1875], Loss: 0.08901456416348616\n",
      "Epoch [6/10], Step [1000/1875], Loss: 0.09862237431108951\n",
      "Epoch [6/10], Step [1100/1875], Loss: 0.1091313164204359\n",
      "Epoch [6/10], Step [1200/1875], Loss: 0.11869948060711225\n",
      "Epoch [6/10], Step [1300/1875], Loss: 0.12864488708476227\n",
      "Epoch [6/10], Step [1400/1875], Loss: 0.13849875832398734\n",
      "Epoch [6/10], Step [1500/1875], Loss: 0.14823979730308057\n",
      "Epoch [6/10], Step [1600/1875], Loss: 0.158082765502731\n",
      "Epoch [6/10], Step [1700/1875], Loss: 0.16770289755860965\n",
      "Epoch [6/10], Step [1800/1875], Loss: 0.17585868955353895\n",
      "Epoch [7/10], Step [100/1875], Loss: 0.009279889445503552\n",
      "Epoch [7/10], Step [200/1875], Loss: 0.018272770752509434\n",
      "Epoch [7/10], Step [300/1875], Loss: 0.0282010211567084\n",
      "Epoch [7/10], Step [400/1875], Loss: 0.038116533049941066\n",
      "Epoch [7/10], Step [500/1875], Loss: 0.04622794987857342\n",
      "Epoch [7/10], Step [600/1875], Loss: 0.05578279648621877\n",
      "Epoch [7/10], Step [700/1875], Loss: 0.06410790587365628\n",
      "Epoch [7/10], Step [800/1875], Loss: 0.07310531951685746\n",
      "Epoch [7/10], Step [900/1875], Loss: 0.08148956458071868\n",
      "Epoch [7/10], Step [1000/1875], Loss: 0.09086046733856201\n",
      "Epoch [7/10], Step [1100/1875], Loss: 0.09990603872984648\n",
      "Epoch [7/10], Step [1200/1875], Loss: 0.10793748381286859\n",
      "Epoch [7/10], Step [1300/1875], Loss: 0.11774954520910978\n",
      "Epoch [7/10], Step [1400/1875], Loss: 0.12674156090964875\n",
      "Epoch [7/10], Step [1500/1875], Loss: 0.1358448867922028\n",
      "Epoch [7/10], Step [1600/1875], Loss: 0.14435505426377057\n",
      "Epoch [7/10], Step [1700/1875], Loss: 0.15324459548344216\n",
      "Epoch [7/10], Step [1800/1875], Loss: 0.16212125802586477\n",
      "Epoch [8/10], Step [100/1875], Loss: 0.008294394534826279\n",
      "Epoch [8/10], Step [200/1875], Loss: 0.017660946607589722\n",
      "Epoch [8/10], Step [300/1875], Loss: 0.026455352138479552\n",
      "Epoch [8/10], Step [400/1875], Loss: 0.03630904598732789\n",
      "Epoch [8/10], Step [500/1875], Loss: 0.04421514483392239\n",
      "Epoch [8/10], Step [600/1875], Loss: 0.052659482008218766\n",
      "Epoch [8/10], Step [700/1875], Loss: 0.061682657675941784\n",
      "Epoch [8/10], Step [800/1875], Loss: 0.06945515567362308\n",
      "Epoch [8/10], Step [900/1875], Loss: 0.07895061480303606\n",
      "Epoch [8/10], Step [1000/1875], Loss: 0.08817767190337181\n",
      "Epoch [8/10], Step [1100/1875], Loss: 0.09652274451553822\n",
      "Epoch [8/10], Step [1200/1875], Loss: 0.10479457631409168\n",
      "Epoch [8/10], Step [1300/1875], Loss: 0.11425969049433868\n",
      "Epoch [8/10], Step [1400/1875], Loss: 0.12264825708766779\n",
      "Epoch [8/10], Step [1500/1875], Loss: 0.13054340891838073\n",
      "Epoch [8/10], Step [1600/1875], Loss: 0.13765340436200302\n",
      "Epoch [8/10], Step [1700/1875], Loss: 0.1463162873029709\n",
      "Epoch [8/10], Step [1800/1875], Loss: 0.15513301469385624\n",
      "Epoch [9/10], Step [100/1875], Loss: 0.008896216617027919\n",
      "Epoch [9/10], Step [200/1875], Loss: 0.017332329771916073\n",
      "Epoch [9/10], Step [300/1875], Loss: 0.025762258060773213\n",
      "Epoch [9/10], Step [400/1875], Loss: 0.03279583757817745\n",
      "Epoch [9/10], Step [500/1875], Loss: 0.04197117516001066\n",
      "Epoch [9/10], Step [600/1875], Loss: 0.050619296088814734\n",
      "Epoch [9/10], Step [700/1875], Loss: 0.059158795668681464\n",
      "Epoch [9/10], Step [800/1875], Loss: 0.06719558413525423\n",
      "Epoch [9/10], Step [900/1875], Loss: 0.07556713493466377\n",
      "Epoch [9/10], Step [1000/1875], Loss: 0.08412652785579364\n",
      "Epoch [9/10], Step [1100/1875], Loss: 0.09325048013130824\n",
      "Epoch [9/10], Step [1200/1875], Loss: 0.10120162781576315\n",
      "Epoch [9/10], Step [1300/1875], Loss: 0.10984561760077874\n",
      "Epoch [9/10], Step [1400/1875], Loss: 0.11799797288328409\n",
      "Epoch [9/10], Step [1500/1875], Loss: 0.12577618679255248\n",
      "Epoch [9/10], Step [1600/1875], Loss: 0.13494445343563954\n",
      "Epoch [9/10], Step [1700/1875], Loss: 0.14267704937507708\n",
      "Epoch [9/10], Step [1800/1875], Loss: 0.15079398106386263\n",
      "Epoch [10/10], Step [100/1875], Loss: 0.008550924852490425\n",
      "Epoch [10/10], Step [200/1875], Loss: 0.015961928910017013\n",
      "Epoch [10/10], Step [300/1875], Loss: 0.0240797719185551\n",
      "Epoch [10/10], Step [400/1875], Loss: 0.03280716557751099\n",
      "Epoch [10/10], Step [500/1875], Loss: 0.03998124620467424\n",
      "Epoch [10/10], Step [600/1875], Loss: 0.04784194109886885\n",
      "Epoch [10/10], Step [700/1875], Loss: 0.055813372014462945\n",
      "Epoch [10/10], Step [800/1875], Loss: 0.06271043447554112\n",
      "Epoch [10/10], Step [900/1875], Loss: 0.07143153960605463\n",
      "Epoch [10/10], Step [1000/1875], Loss: 0.07946030563016733\n",
      "Epoch [10/10], Step [1100/1875], Loss: 0.08724592570314804\n",
      "Epoch [10/10], Step [1200/1875], Loss: 0.09602391985108455\n",
      "Epoch [10/10], Step [1300/1875], Loss: 0.10454755814721187\n",
      "Epoch [10/10], Step [1400/1875], Loss: 0.11236560085664193\n",
      "Epoch [10/10], Step [1500/1875], Loss: 0.1212306586648027\n",
      "Epoch [10/10], Step [1600/1875], Loss: 0.12994915057619413\n",
      "Epoch [10/10], Step [1700/1875], Loss: 0.1380455621202787\n",
      "Epoch [10/10], Step [1800/1875], Loss: 0.14623369132876396\n",
      "Finished Training!\n",
      "Accuracy: 97.16% Test Loss: 0.0948 Precision: 0.9717 Recall: 0.9716 F1 Score: 0.9716 Fit Time: 143.4\n",
      "------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, net in enumerate(networks):\n",
    "    print(f\"Treinando Rede Neural {i + 1} com os seguintes parâmetros:\")\n",
    "    print(grid[i])\n",
    "\n",
    "    start = time.time()\n",
    "    train_model(net, criterion, optimizers[i], epochs)\n",
    "    end = time.time()\n",
    "    total_time = round(end - start, 1)\n",
    "\n",
    "    accuracy, loss, precision, recall, f1 = test_model(net, criterion)\n",
    "    print(f'Accuracy: {100 * accuracy}% Test Loss: {loss:.4f} Precision: {precision:.4f} Recall: {recall:.4f} F1 Score: {f1:.4f} Fit Time: {total_time}')\n",
    "    print('------------------------------------\\n')\n",
    "\n",
    "    class_accuracies = accuracy_classes(net)\n",
    "\n",
    "    model_data  = {\n",
    "        'network': [], #colocar como é a rede, podemos criar um nome pra cada rede e por ou algo do genero\n",
    "        **grid[i],\n",
    "        'fit_time': total_time,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'overall_accuracy ': accuracy,\n",
    "        'loss': loss,\n",
    "        'total_epochs': epochs,\n",
    "        'learning_rate': learning_rate,\n",
    "    }\n",
    "\n",
    "    for idx, acc in enumerate(class_accuracies):\n",
    "        model_data[f'accuracy_{idx}'] = acc\n",
    "\n",
    "    # Adicionando o modelo ao dataframe de modelos\n",
    "    model_df.loc[len(model_df)] = model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df.to_csv('cnn_models_metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando modelo treinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseCNN(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=6272, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# path_model_trained = ''\n",
    "\n",
    "model_trained = BaseCNN().to(device)\n",
    "model_trained.load_state_dict(torch.load(path_model_trained))\n",
    "model_trained.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prevendo a classe de uma entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classe prevista: 9\n"
     ]
    }
   ],
   "source": [
    "image_path = 'imagens_teste\\imagem_teste_4.png'\n",
    "\n",
    "predict_image(image_path, model_trained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analisando acurácia de cada classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for class: 0 is 99.18 %\n",
      "Accuracy for class: 1 is 99.12 %\n",
      "Accuracy for class: 2 is 97.29 %\n",
      "Accuracy for class: 3 is 98.12 %\n",
      "Accuracy for class: 4 is 98.37 %\n",
      "Accuracy for class: 5 is 97.87 %\n",
      "Accuracy for class: 6 is 97.70 %\n",
      "Accuracy for class: 7 is 97.37 %\n",
      "Accuracy for class: 8 is 97.23 %\n",
      "Accuracy for class: 9 is 96.53 %\n"
     ]
    }
   ],
   "source": [
    "all_accuracy = accuracy_classes(model_trained)\n",
    "for i, accuracy in enumerate(all_accuracy):\n",
    "    print(f'Accuracy for class: {i} is {accuracy:.2f} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
