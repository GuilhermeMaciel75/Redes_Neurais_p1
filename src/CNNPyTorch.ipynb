{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os, datetime, torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms \n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "import seaborn as sns\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "import inspect\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando e seperando dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# Transformações para normalizar o dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Carregando o dataset MNIST\n",
    "trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# DataLoader para o dataset de treinamento e teste\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "classes = list(map(str, range(0, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualizando dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(dl):\n",
    "    for images, labels in dl:\n",
    "        fig, ax = plt.subplots(figsize=(12, 12))\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "        ax.imshow(make_grid(images[:64], nrow=8).permute(1, 2, 0))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_batch(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_batch(testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando uma instância da classe nn.Module para criar redes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explicando cada parâmetro da camada convolucional:\n",
    "\n",
    "- **in_channels**: Quantos canais de cores os inputs possuem. No caso de imagens em preto e branco, como estamos trabalhando com MNIST, há apenas um canal de cor.\n",
    "\n",
    "- **out_channels**: Número de filtros (kernels) que serão aplicados à imagem durante a convolução. Cada filtro é responsável por extrair características latentes da imagem.\n",
    "\n",
    "- **kernel_size**: Dimensão do filtro utilizado na convolução. O valor comum de 3 é amplamente usado, pois é suficiente para capturar detalhes locais, ao mesmo tempo em que percebe padrões maiores na imagem.\n",
    "\n",
    "- **stride**: Indica de quanto em quantos pixels o filtro será aplicado na imagem.\n",
    "\n",
    "- **padding**: Adiciona pixels em volta da imagem de entrada durante a convolução para garantir que o tamanho da saída após a convolução permaneça o mesmo que o da entrada.\n",
    "  - *Observação*: Ao adicionar padding, é preciso ter cuidado com os valores para garantir que a resolução e o tamanho da imagem não sejam afetados de forma indesejada.\n",
    "\n",
    "- **dropout_prob**: Probabilidade de que um neurônio seja desligado durante o treinamento da rede. Isso é uma técnica de regularização que ajuda a prevenir o overfitting, forçando a rede a aprender representações mais robustas e generalizáveis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fórmulas das Dimensões de Saída\n",
    "\n",
    "A fórmula para calcular a dimensão de saída \\( W_out \\) da convolução é:\n",
    "\n",
    "$$\n",
    "W_{\\text{out}} = \\left\\lfloor \\frac{W_{\\text{in}} + 2 \\times \\text{padding} - \\text{kernel\\_size}}{\\text{stride}} \\right\\rfloor + 1\n",
    "$$\n",
    "\n",
    "E a fórmula para calcular a dimensão de saída \\( H_out \\) da convolução é:\n",
    "\n",
    "$$\n",
    "H_{\\text{out}} = \\left\\lfloor \\frac{H_{\\text{in}} + 2 \\times \\text{padding} - \\text{kernel\\_size}}{\\text{stride}} \\right\\rfloor + 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resumindo as trasnformações da BaseCNN:\n",
    "\n",
    "- Entrada Original: 28x28 pixels, 1 canal.\n",
    "- Após Convolução: 28x28 pixels, 32 canais.\n",
    "- Após Pooling: 14x14 pixels, 32 canais.\n",
    "- Entrada para self.fc1: 32 × 14 × 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A BaseCNN é uma implementação básica de uma rede neural convolucional (CNN). Ela possui uma camada de convolução, uma camada de max pooling, uma camada de dropout e uma camada fully connected (linear). Esta arquitetura foi desenvolvida para experimentos iniciais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseCNN(nn.Module):\n",
    "    def __init__(self, conv_kernel_size=3, conv_stride=1, conv_padding=1, pool_kernel_size=2, pool_stride=2, dropout_prob=0.5):\n",
    "        super(BaseCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=conv_kernel_size, stride=conv_stride, padding=conv_padding)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=pool_kernel_size, stride=pool_stride)\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        self.fc1 = nn.Linear(32 * 14 * 14, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(-1, 32 * 14 * 14)\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Já a classe CNN é uma rede neural convolucional mais complexa que leva em consideração os hiperparâmetros sugeridos pelo Optuna para otimização automática. Ela possui várias camadas convolucionais, uma camada de pooling, camadas fully connected (lineares) e camadas de dropout. O método calc_conv_output_size é usado para calcular o tamanho da saída após as operações de convolução e pooling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A rede começa com a entrada passando por camadas convolucionais e max pooling, onde os dados são filtrados e as dimensões são reduzidas. Depois, um dropout 2D é aplicado após a terceira camada convolucional para regularizar o treinamento. Os dados são então remodelados para as camadas fully connected, onde são aplicadas ativações ReLU e dropout antes de passar para a camada final que gera as previsões. Antes de retornar as previsões, uma função Log Softmax é aplicada para normalizar as saídas e gerar uma distribuição de probabilidade sobre as classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def _calc_conv_output_size(self, in_size, kernel_size, conv_stride, pool_size, pool_stride, conv_padding):\n",
    "        # Apply convolution\n",
    "        out_size = (in_size + 2 * conv_padding - kernel_size) // conv_stride + 1\n",
    "        # Apply pooling\n",
    "        out_size = (out_size - pool_size) // pool_stride + 1\n",
    "        return out_size\n",
    "\n",
    "    def __init__(self, num_conv_layers, num_filters, num_neurons, drop_conv2, drop_prob_fc1,\n",
    "                 conv_kernel_size=3, pool_kernel_size=2, conv_stride=1, pool_stride=2, conv_padding=0):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        in_size = 28\n",
    "\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, num_filters[0],\n",
    "                                              kernel_size=(conv_kernel_size, conv_kernel_size),\n",
    "                                              stride=conv_stride, padding=conv_padding)])\n",
    "        out_size = self._calc_conv_output_size(in_size, conv_kernel_size, conv_stride,\n",
    "                                               pool_kernel_size, pool_stride, conv_padding)\n",
    "\n",
    "        for i in range(1, num_conv_layers):\n",
    "            self.convs.append(nn.Conv2d(in_channels=num_filters[i - 1], out_channels=num_filters[i],\n",
    "                                        kernel_size=(conv_kernel_size, conv_kernel_size),\n",
    "                                        stride=conv_stride, padding=conv_padding))\n",
    "            out_size = self._calc_conv_output_size(out_size, conv_kernel_size, conv_stride,\n",
    "                                                   pool_kernel_size, pool_stride, conv_padding)\n",
    "\n",
    "        if out_size <= 0:\n",
    "            raise ValueError(\"Output size is too small after convolution and pooling operations. \" +\n",
    "                             \"Adjust the kernel sizes and strides.\")\n",
    "\n",
    "        self.conv2_drop = nn.Dropout2d(p=drop_conv2)\n",
    "        self.out_feature = num_filters[num_conv_layers - 1] * out_size * out_size\n",
    "        self.pool = nn.MaxPool2d(kernel_size=pool_kernel_size, stride=pool_stride)\n",
    "        self.fc1 = nn.Linear(self.out_feature, num_neurons)\n",
    "        self.fc2 = nn.Linear(num_neurons, 10)\n",
    "        self.fc1_drop_prob = drop_prob_fc1\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x)\n",
    "            if i == 2:\n",
    "                x = self.conv2_drop(x)\n",
    "            x = F.relu(self.pool(x))\n",
    "\n",
    "        x = x.view(-1, self.out_feature)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, p=self.fc1_drop_prob, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definindo funções úteis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Função de treinamento do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, epochs):\n",
    "    model.train().to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for inputs, labels in trainloader:\n",
    "            (inputs, labels) = (inputs.to(device), labels.to(device))\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch {epoch + 1}, Loss: {running_loss / len(trainloader)}')\n",
    "    \n",
    "    print('Finished Training!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Função de teste do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, criterion):\n",
    "    model.eval().to(device)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    accuracy = correct / total\n",
    "    loss = test_loss / len(testloader)\n",
    "    precision = precision_score(all_labels, all_predictions, average='weighted')\n",
    "    recall = recall_score(all_labels, all_predictions, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "\n",
    "    return accuracy, loss, precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Função de acurácia de cada classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_classes(model_trained):\n",
    "    correct_pred = {classname: 0 for classname in classes}\n",
    "    total_pred = {classname: 0 for classname in classes}\n",
    "    all_accuracy = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model_trained(images)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            \n",
    "            for label, prediction in zip(labels, predictions):\n",
    "                if label == prediction:\n",
    "                    correct_pred[classes[label]] += 1\n",
    "                total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "    for classname, correct_count in correct_pred.items():\n",
    "        accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "        all_accuracy.append(accuracy)\n",
    "    \n",
    "    return all_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Função para prever classe de uma imagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(image_path, model):\n",
    "    image = Image.open(image_path).convert('L')  \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((28, 28)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    image = transform(image).unsqueeze(0) \n",
    "\n",
    "    output = model(image).to(device)\n",
    "\n",
    "    _, predicted_class = torch.max(output, 1)\n",
    "    print(\"Classe prevista:\", predicted_class.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Função de treinamento por epoca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_once(model, criterion, optimizer):\n",
    "    model.train().to(device)\n",
    "\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in trainloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    " \n",
    "    return running_loss / len(trainloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning de Hiperparâmetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em seguida, realizamos 10 experimentos, cada um com parâmetros diferentes escolhidos usando a biblioteca Optuna, para otimização dos hiperparâmetros. Para cada experimento, iremos salvar seus resultados (acurácia total, perda de teste, precisão, recall e f1 score), e quais os parâmetros utlizados (número de camadas convolucionais, número de filtros, número de neurônios, dropout para a segunda camada convolucional, probabilidade de dropout para a primeira camada totalmente conectada, tamanho do kernel convolucional, stride convolucional, padding convolucional, tamanho do kernel de pooling, stride de pooling).\n",
    "<br>No final de cada experimento, são salvos os três modelos com a maior acurácia total, os quais são exibidos após a conclusão de todos os 10 testes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    trial.set_user_attr(\"network\", type(model_class).__name__)\n",
    "    trial.set_user_attr(\"total_epochs\", num_epochs)\n",
    "\n",
    "    # Define range of values to be tested for the hyperparameters\n",
    "    trial_params = {\n",
    "        \"num_conv_layers\": trial.suggest_int(\"num_conv_layers\", 1, 3),\n",
    "        \"num_filters\": [int(trial.suggest_discrete_uniform(\"num_filter_\" + str(i), 16, 128, 16))\n",
    "                        for i in range(trial.suggest_int(\"num_conv_layers\", 1, 3))],\n",
    "        \"num_neurons\": trial.suggest_int(\"num_neurons\", 10, 400, 10),       # Number of neurons of fully connected layer 1\n",
    "        \"drop_conv2\": trial.suggest_float(\"drop_conv2\", 0.2, 0.5),          # Dropout for convolutional layer 2\n",
    "        \"drop_prob_fc1\": trial.suggest_float(\"drop_prob_fc1\", 0.2, 0.5),    # Dropout probability for fully connected layer 1\n",
    "        \"conv_kernel_size\": trial.suggest_int(\"conv_kernel_size\", 3, 7, step=2),\n",
    "        \"conv_stride\": trial.suggest_int(\"conv_stride\", 1, 2),\n",
    "        \"conv_padding\": trial.suggest_int(\"conv_padding\", 1, 3),\n",
    "        \"pool_kernel_size\": trial.suggest_int(\"pool_kernel_size\", 2, 3),\n",
    "        \"pool_stride\": trial.suggest_int(\"pool_stride\", 2, 3),\n",
    "        \"dropout_prob\": trial.suggest_float(\"drop_prob_fc1\", 0.2, 0.5)\n",
    "    }\n",
    "\n",
    "    # Manually add these parameters to the trial's attributes\n",
    "    trial.set_user_attr(\"conv_kernel_size\", trial_params[\"conv_kernel_size\"])\n",
    "    trial.set_user_attr(\"conv_stride\", trial_params[\"conv_stride\"])\n",
    "    trial.set_user_attr(\"conv_padding\", trial_params[\"conv_padding\"])\n",
    "    trial.set_user_attr(\"pool_kernel_size\", trial_params[\"pool_kernel_size\"])\n",
    "    trial.set_user_attr(\"pool_stride\", trial_params[\"pool_stride\"])\n",
    "\n",
    "  \n",
    "    # Generate the model\n",
    "    model_params = inspect.signature(model_class.__init__).parameters\n",
    "    model_init_args = {}\n",
    "    \n",
    "    for param_name, param in model_params.items():\n",
    "        if param_name == 'self':\n",
    "            continue\n",
    "        else:\n",
    "            model_init_args[param_name] = trial_params[param_name]\n",
    "\n",
    "    try:\n",
    "        model = model_class(**model_init_args).to(device)\n",
    "\n",
    "        # Generate the optimizer\n",
    "        optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n",
    "        lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
    "        optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
    "\n",
    "        # Train the model\n",
    "        for epoch in range(num_epochs):\n",
    "            train_loss = train_model_once(model, criterion, optimizer)\n",
    "            accuracy, test_loss, precision, recall, f1 = test_model(model, criterion)\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}, \" +\n",
    "                f\"Train Loss: {train_loss:.4f}, \" + \n",
    "                f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "            # Pruning (stops trial early if not promising)\n",
    "            trial.report(accuracy, epoch)\n",
    "            # Handle pruning based on the intermediate value.\n",
    "            if trial.should_prune():\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        current_datetime = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        path_model_trained = os.path.join('modelos_treinados', f'optune{current_datetime}.pth')\n",
    "        torch.save(model.state_dict(), path_model_trained)\n",
    "\n",
    "        trial.set_user_attr(\"accuracy\", accuracy)\n",
    "        trial.set_user_attr(\"test_loss\", test_loss)\n",
    "        trial.set_user_attr(\"precision\", precision)\n",
    "        trial.set_user_attr(\"recall\", recall)\n",
    "        trial.set_user_attr(\"path_model_trained\", path_model_trained)\n",
    "\n",
    "        \n",
    "        return accuracy\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando nossa rede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class = BaseCNN #or other class\n",
    "num_epochs = 10\n",
    "num_trials = 10\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=num_trials)\n",
    "\n",
    "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Savando os resultados obtidos no csv e ordenando por acurácia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = study.trials_dataframe()\n",
    "df = df[df[\"state\"] == \"COMPLETE\"].drop([\"datetime_start\", \"datetime_complete\", \"duration\", \"state\"], axis=1)\n",
    "df = df.sort_values(\"value\")  # Sort based on accuracy\n",
    "\n",
    "csv_file = \"optuna_results.csv\"\n",
    "df_existing = pd.read_csv(csv_file) if os.path.isfile(csv_file) else None\n",
    "\n",
    "df_existing = pd.concat([df_existing, df], ignore_index=True) if df_existing is not None else df\n",
    "df_existing.to_csv(csv_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_existing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estatísticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n-- Study Statistics --\")\n",
    "print(f\"  Number of finished trials: {len(study.trials)}\")\n",
    "print(f\"  Number of pruned trials:   {len(pruned_trials)}\")\n",
    "print(f\"  Number of complete trials: {len(complete_trials)}\")\n",
    "\n",
    "best_trial = study.best_trial\n",
    "print(\"\\n-- Best Trial --\")\n",
    "print(f\"  Accuracy:  {best_trial.value}\")\n",
    "print(f\"  Test Loss: {best_trial.user_attrs['test_loss']}\")\n",
    "print(f\"  Precision: {best_trial.user_attrs['precision']}\")\n",
    "print(f\"  Recall:    {best_trial.user_attrs['recall']}\")\n",
    "print(f\"  F1 Score:  {best_trial.user_attrs['f1_score']}\")\n",
    "print(\"  Parameters: \")\n",
    "for key, val in best_trial.params.items():\n",
    "    print(f\"    {key}: {(16 - len(key)) * ' '}{val}\")\n",
    "print(f\"  conv_kernel_size: {best_trial.user_attrs['conv_kernel_size']}\")\n",
    "print(f\"  conv_stride:      {best_trial.user_attrs['conv_stride']}\")\n",
    "print(f\"  conv_padding:     {best_trial.user_attrs['conv_padding']}\")\n",
    "print(f\"  pool_kernel_size: {best_trial.user_attrs['pool_kernel_size']}\")\n",
    "print(f\"  pool_stride:      {best_trial.user_attrs['pool_stride']}\")\n",
    "\n",
    "print(f\"\\n-- Overall Results (Ordered by Accuracy) --\")\n",
    "print(df)\n",
    "\n",
    "most_important_parameters = optuna.importance.get_param_importances(study, target=None)\n",
    "\n",
    "print(\"\\n-- Most Important Hyperparameters --\")\n",
    "for key, val in most_important_parameters.items():\n",
    "    print(f\"  {key}: {(15 - len(key)) * ' '}{(100 * val):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instânciando o modelo base, definindo parametros, loss function e otimizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaseCNN().to(device)\n",
    "\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treinamento o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, criterion, optimizer, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, loss, precision, recall, f1 = test_model(model, criterion)\n",
    "print(f'Accuracy: {100 * accuracy}%, Test Loss: {loss:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Salvando o modelo treinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('modelos_treinados'):\n",
    "    os.makedirs('modelos_treinados')\n",
    "\n",
    "current_datetime = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "path_model_trained = os.path.join('modelos_treinados', f'model_{current_datetime}.pth')\n",
    "\n",
    "torch.save(model.state_dict(), path_model_trained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando modelo treinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model_trained = 'modelos_treinados\\optune20240522_025008.pth'\n",
    "path_csv = \"src\\optuna_results.csv\"\n",
    "df = pd.read_csv(path_csv)\n",
    "\n",
    "row = df[df['path_model_trained'] == path_model_trained]\n",
    "className = row['network'].iloc[0]\n",
    "ModelClass = getattr(importlib.import_module('__main__'), className)\n",
    "\n",
    "model_trained = ModelClass().to(device)\n",
    "model_trained.load_state_dict(torch.load(path_model_trained))\n",
    "model_trained.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analisando acurácia de cada classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_accuracy = accuracy_classes(model_trained)\n",
    "for i, accuracy in enumerate(all_accuracy):\n",
    "    print(f'Accuracy for class: {i} is {accuracy:.2f} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise dos Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('optuna_results.csv')\n",
    "df.sort_values(by=['number'], inplace=True)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(df['number']))\n",
    "width = 0.2  # Largura das barras\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "bars4 = ax.bar(x - width, df['user_attrs_accuracy'], width - 0.01, label='Acurácia', color=(0.9, 0.41, 0.38))\n",
    "bars2 = ax.bar(x, df['user_attrs_precision'], width - 0.01, label='Precisão', color=(1.0, 0.8, 0.6, 0.8))\n",
    "bars1 = ax.bar(x + width, df['user_attrs_recall'], width - 0.01, label='Recall', color=(0.6, 0.8, 1.0, 0.8))\n",
    "bars3 = ax.bar(x + 2*width, df['user_attrs_f1_score'], width - 0.01, label='F1-Score', color=(0.6, 1.0, 0.8, 0.8))\n",
    "\n",
    "ax.set_xlabel('Modelos')\n",
    "ax.set_ylabel('Métricas')\n",
    "ax.set_title('Métricas de Avaliação de Modelos')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(df['number'])\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "bars4 = ax.bar(x - width, df['user_attrs_accuracy'], width - 0.01, label='Acurácia', color=(0.9, 0.41, 0.38))\n",
    "bars2 = ax.bar(x, df['user_attrs_precision'], width - 0.01, label='Precisão', color=(1.0, 0.8, 0.6, 0.8))\n",
    "bars1 = ax.bar(x + width, df['user_attrs_recall'], width - 0.01, label='Recall', color=(0.6, 0.8, 1.0, 0.8))\n",
    "bars3 = ax.bar(x + 2*width, df['user_attrs_f1_score'], width - 0.01, label='F1-Score', color=(0.6, 1.0, 0.8, 0.8))\n",
    "\n",
    "ax.set_xlabel('Modelos')\n",
    "ax.set_ylabel('Métricas')\n",
    "ax.set_title('Métricas de Avaliação de Modelos')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(df['number'])\n",
    "ax.set_ylim(0.9, 1.0)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_data = pd.DataFrame(all_accuracy, columns=['Acurácia'])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(acc_data.index, acc_data['Acurácia'], color='skyblue')\n",
    "\n",
    "# Adicionando títulos e rótulos\n",
    "plt.title('Acurácia por Classe')\n",
    "plt.xlabel('Classe')\n",
    "plt.ylabel('Acurácia (%)')\n",
    "plt.xticks(acc_data.index)\n",
    "plt.ylim(80, 100)\n",
    "\n",
    "# Mostrando o gráfico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupar por otimizador e calcular médias\n",
    "optimizer_group = df.groupby('params_optimizer').mean()\n",
    "\n",
    "r1 = np.arange(len(optimizer_group.index))\n",
    "r2 = [x + width for x in r1]\n",
    "r3 = [x + width for x in r2]\n",
    "\n",
    "\n",
    "# Gráfico de barras\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Precisão\n",
    "plt.bar(r1, optimizer_group['user_attrs_precision'], width - 0.01, label='Precisão', alpha=0.6)\n",
    "\n",
    "# Recall\n",
    "plt.bar(r2, optimizer_group['user_attrs_recall'], width - 0.01, label='Recall', alpha=0.6)\n",
    "\n",
    "# F1-score\n",
    "plt.bar(r3, optimizer_group['user_attrs_f1_score'], width - 0.01, label='F1-Score', alpha=0.6)\n",
    "\n",
    "plt.xlabel('Otimizador')\n",
    "plt.ylabel('Média dos Valores')\n",
    "plt.title('Média de Precisão, Recall e F1-Score para Cada Otimizador')\n",
    "plt.xticks([r + width for r in range(len(optimizer_group.index))], optimizer_group.index)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df['number'], df['user_attrs_test_loss'], marker='o', linestyle='-', color='red')\n",
    "plt.xlabel('Número do Experimento')\n",
    "plt.ylabel('Perda de Teste')\n",
    "plt.title('Perda de Teste vs. Número do Experimento')\n",
    "plt.legend(['Teste'])\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "sns.scatterplot(data=df, x='params_num_conv_layers', y='user_attrs_accuracy', hue='params_optimizer', style='params_optimizer', markers=True,\n",
    "                palette='viridis', s=100)\n",
    "\n",
    "plt.title('Comparação de Acurácia dos Modelos por Quantidade de Camadas Convolucionais')\n",
    "plt.xlabel('Número de Camadas Convolucionais')\n",
    "plt.ylabel('Precisão')\n",
    "plt.xticks(df['params_num_conv_layers'].unique())\n",
    "plt.grid(True)\n",
    "plt.legend(title='Tipo de otimizador', bbox_to_anchor=(1.05, 1), loc='upper left')  # Movendo a legenda para fora do gráfico\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(df.groupby('params_num_conv_layers')['user_attrs_accuracy'].mean().reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "sns.scatterplot(data=data, x='params_num_conv_layers', y='user_attrs_accuracy', markers=True,\n",
    "                 s=100)\n",
    "\n",
    "plt.title('Média da Acurácia por Quantidade de Camadas Convolucionais')\n",
    "plt.xlabel('Número de Camadas Convolucionais')\n",
    "plt.ylabel('Precisão')\n",
    "plt.xticks(df['params_num_conv_layers'].unique())\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='params_num_conv_layers', hue='params_optimizer', data=df)\n",
    "plt.title('Distribuição das Camadas Convolucionais por Otimizador')\n",
    "plt.xlabel('Número de Camadas Convolucionais')\n",
    "plt.ylabel('Contagem')\n",
    "plt.legend(title='Otimizador')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "sns.scatterplot(data=df, x='params_drop_conv2', y='user_attrs_accuracy', hue='params_num_conv_layers', palette='viridis'\n",
    "                , s=100)\n",
    "\n",
    "plt.title('Acurácia por Taxa de Dropout')\n",
    "plt.xlabel('Taxa de dropout')\n",
    "plt.ylabel('Acurácia')\n",
    "plt.xlim(0.0, 1)\n",
    "plt.grid(True)\n",
    "plt.legend(title='Número de Camadas Convolucionais', loc='upper right')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
